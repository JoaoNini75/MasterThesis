%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter3.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother


\chapter{State of the Art}
\label{cha:state_of_the_art}



\section{Self-composition} 
\label{sec:self_composition}

The proposal of self-composition in this~\cite{DBLP:conf/csfw/BartheDR04} work was to offer an extensible and flexible way of controlling information flow.
This is usually done by information flow type systems, but they suffer from the issues that self-composition attempted to solve.
In the work mentioned above, the authors were trying to address the static enforcement of information flow policies but focused on a specific one called non-interference.
Non-interference separates the state of a program into a public and a secret part and, by observing its execution, determines if there was a information leak on the secret part of the state.

Despite the well-defined focus, the efforts of the work mentioned in the beginning of the section apply to several programming languages and different definitions of security, even including declassification, partially.
In the end, the authors were able to establish a general theory of self-composition to prove that programs are non-interfering.
One of the main features of self-composition is its expressiveness and that there is no need to prove the soundness of type systems, since it is based on logic.


\section{Cross-products} 
\label{sec:cross_products}

Cross-products~\cite{DBLP:conf/fm/ZaksP08} aim to prove program equivalence, with a strong focus on verifying compiler optimizations.
Instead of proving that both programs are equivalent, the analysis is done by combining the two input programs into one: the cross-product.
With them, instead of recurring to program analysis and proof rules developed specifically for translation validation, it became possible to use existing methods and tools to prove properties of a single program. 
Despite handling the most common intraprocedural compiler optimizations, cross-products can not be applied to two input programs with dissimilar structures, which is a major constraint for this work.

One important aspect of CoVaC, a framework developed by the authors of the aforementioned article, was made to validate program equivalence in general while balacing precision and efficiency.
The approach was to divide the analysis in two phases, a faster one first and a more precise after that.
The first phase utilizes a fast, yet imprecise value numbering algorithm whose results are used to determine if it is necessary to apply the second phase. 
This final part is based on assertion checking, a static program verification method which, under the hood, computes the weakest-precondition~\cite{DBLP:books/ph/Dijkstra76} using CVC3~\cite{cvc3}. 

The results presented on the cited work reveal that CoVaC was able to verify a very considerable set of optimizations of LLVM~\cite{llvm}, a complex modern compiler.
Yet, the tool does not support verification of interprocedural optimizations or information flow policies, for example, but these are issues that the authors showed interest in addressing.


\section{Product Programs} 
\label{sec:product_programs}

\subsection{Motivation} 
\label{subsec:product_programs_motivation}

Relational Hoare Logic serves as a good starting point to compare the behaviour of two different executions of the same program or even two different programs.
However, there are few available tools and practical reasoning logics for relational verification.
One of the main limitations of the existing ones~\cite{DBLP:conf/popl/Benton04, DBLP:journals/tcs/Yang07} is the constraint of \emph{structural equivalence}.
Inversely, traditional program verification has diverse and extense tool support.
Therefore, as a way of getting around that obstacle, relational verification tasks could be soundly reduced into standard verification.
This means we would translate Hoare quadruples (\{$\Phi$\} \emph{P1} \emph{\textasciitilde} \emph{P2} \{$\Psi$\}) into Hoare triples (\{$\phi$\} \emph{P} \{$\psi$\}), making the triples valid when the quadruples are also valid.
Considering $\vDash$ the symbol for validity, the objective is finding \emph{$\phi$, P, $\psi$} that:
\[ \vDash \{\phi\} \, \textbf{P} \, \{\psi\} \ \ \ \rightarrow \ \ \ \ \ \vDash \{\Phi\} \, \textbf{P1} \sim \textbf{P2} \, \{\Psi\} \]

Let us consider two imperative and \emph{separable} programs \emph{P1} and \emph{P2}.
This enables the capacity of the assertions to be seen as first-order formulas about the variables of \emph{P1} and \emph{P2}.
Self-composition~\cite{DBLP:conf/csfw/BartheDR04} comes from the establishment of the wanted equalities above: \emph{P $\equiv$ P1;P2}, \emph{$\phi$ $\equiv$ $\Phi$} and \emph{$\psi$ $\equiv$ $\Psi$}.
Despite being sound and considerably complete, this construction is impractical~\cite{DBLP:conf/sas/TerauchiA05} for two main reasons.
One of them is that the existing automatic safety analysis tools available are not powerful enough to verify most of realistic problems and, when they can, there is a lack of performance.
The other reason is that, for the authors of the last cited work, the safety analysis developed for naturally 1-safety problems is not expected to advance significantly in the foreseeable future.

There is another relevant method discussed in this \hyperref[sec:cross_products]{section}, the cross-products.
These suffer from the constraint of structural equivalence, making impossible to reason about properties or program optimizations that are based on different control flows.

Product Programs~\cite{DBLP:conf/fm/BartheCK11} appear to be the best path to follow, since they represent a general notion that combines the flexibility of self-composition in executing asynchronous steps and the efficiency of cross-products when it comes to treat synchronous steps. 


\subsection{From relational verification to standard verification} 
\label{subsec:product_programs_relverif_to_stdverif}

Consider two generic programs \emph{P1} and \emph{P2} and their product program \emph{P}.
The approach of product programs to reduce relational reasoning into standard reasoning goes through the capacity of constructing \emph{P}, that simulates the execution of both \emph{P1} and \emph{P2}.
Next, we will describe the grammar rules to show how we will proceed and then explain how product programs are effectively constructed.

\subsubsection{Establishing the ground rules} 
\label{subsubsec:product_programs_ground_rules}

The commands in our programming model will stick to these grammar rules:
\[ \text{c ::=} \ \ \ \text{x:= e} \ | \ \text{a[e]:= e} \ | \ \textbf{skip} \ | \ \textbf{assert} \text{(b)} \ |
                    \ \text{c; c} \ | \ \textbf{if} \ \text{b} \ \textbf{then} \ \text{c1} \ \textbf{else} \ \text{c2} \ | \ \textbf{while} \ \text{b} \ \textbf{do} \ \text{c} \]

In this context, \emph{x} is a integer variable, \emph{a} is an array variable, \emph{e} is an integer expression and \emph{b} is a boolean expression.
Execution states are represented as \emph{S = (V$_i$ $\rightharpoonup$ $\mathbb{Z}$) X (V$_a$ $\rightharpoonup$ ($\mathbb{Z}$ $\rightharpoonup$ $\mathbb{Z}$)).}
The semantics of the commands are standard, deterministic and are based on the relation $\langle$c, $\sigma$$\rangle$ $\rightsquigarrow$ $\langle$c', $\sigma$'$\rangle$.
Notice that $\langle$\textbf{skip}, $\sigma$$\rangle$ marks the end of the programs, \textbf{assert}(b) blocks the execution of the program if \emph{b} is false and we let $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' mean $\langle$c, $\sigma$$\rangle$ $\rightsquigarrow$* $\langle$\textbf{skip}, $\sigma$'$\rangle$.

As we discussed before in this document, two commands are \emph{separable} if they operate on disjoint sets of variables.
Similarly, we consider that two states are \emph{separable} if their domains are disjoint.
Consider $\mu$$_1$ $\uplus$ $\mu$$_2$ represent the union of finite maps:
\[
(\mu_1 \uplus \mu_2) \ x =
\begin{cases}
    \mu_1 x & \text{if } x \in \operatorname{dom}(\mu_1) \\
    \mu_2 x & \text{if } x \in \operatorname{dom}(\mu_2)
\end{cases}
\]

and overload this notation for the union of separable states ($\mu$,$\nu$) $\uplus$ ($\mu$',$\nu$'), whose definition is ($\mu$ $\uplus$ $\mu$', $\nu$ $\uplus$ $\nu$').
Taking into account that we assume that the states are separable, another way of looking at assertions is by viewing them as relations on states: ($\sigma$$_1$, $\sigma$$_2$) $\in$ $\llbracket$$\phi$$\rrbracket$ iff $\sigma$$_1$ $\uplus$ $\sigma$$_2$ $\in$ $\llbracket$$\phi$$\rrbracket$.
Therefore, the definition below illustrates the formal statement of valid relational specifications.
\bigskip

\textbf{Definition 1 -}  \emph{Two commands $c_1$ and $c_2$ satisfy the pre-condition $\phi$ and the post-condition $\psi$ described by a valid Hoare quadruple if,
                                for all states $\sigma$$_1$, $\sigma$$_2$, $\sigma$$_1$', $\sigma$$_2$' such that $\sigma_1$ $\uplus$ $\sigma_2$ $\in$ $\llbracket$$\phi$$\rrbracket$
                                and $\langle$$c_1$, $\sigma$$_1$$\rangle$ $\Downarrow$ $\sigma$$_1$' and $\langle$$c_2$, $\sigma$$_2$$\rangle$ $\Downarrow$ $\sigma$$_2$', we have $\sigma_1$' $\uplus$ $\sigma_2$' $\in$ $\llbracket$$\psi$$\rrbracket$.}

\bigskip
Since our objective is to reduce the validity of Hoare quadruples to validity of Hoare triples, we must say that we establish our notion of valid Hoare triple as stronger than usual.
It requires that the command allows the program to finish its execution, i.e., the command is \emph{non-stuck}.
\bigskip

\textbf{Definition 2 -}  \emph{A Hoare triple {$\phi$} c {$\psi$} is valid ($\vDash$ \{$\phi$\} c \{$\psi$\}) if c is $\phi$-nonstuck and for all $\sigma$, $\sigma$' $\in$ S, $\sigma$ $\in$ $\llbracket$$\phi$$\rrbracket$ and $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' imply $\sigma$' $\in$ $\llbracket$$\psi$$\rrbracket$.}

\bigskip
This notion of validity requires, however, that we extend Hoare logic for it to be able to treat \textbf{assert} statements.
The necessary rule is:
\begin{figure}[h]
  \centering
  \begin{mathpar}
  
  \inferrule*[]
  { }
  {\vdash \ \{ \textit{b} \land \Phi \} \ \textbf{assert}\text{(b)} \ \{ \Phi \}}

  \end{mathpar}
\end{figure}


\subsubsection{Construction of Product Programs} 
\label{subsubsec:product_programs_construction}

Firstly in this section, we define the rules that will be used to deal with structurally equivalent programs. 
After that, we that set with structural transformations to allow the treatment of programs with different structures.

\begin{figure}[H]
  \centering
  \begin{mathpar}

  \inferrule*[]
    { }
    {c_1 \times c_2 \; \rightarrow \; c_1 ; c_2}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c \\ 
     c'_1 \times c'_2 \; \rightarrow \; c'}
    {(c_1 ; c'_1) \times (c_2 ; c'_2) \; \rightarrow \; c ; c'}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c}
    {(\textbf{while} \; b_1 \; \textbf{do} \; c_1) \times (\textbf{while} \; b_2 \; \textbf{do} \; c_2) \; \rightarrow \;
    \textbf{assert}(b_1 \Leftrightarrow b_2); \; \textbf{while} \; b_1 \; \textbf{do} \; (c ; \textbf{assert}(b_1 \Leftrightarrow b_2))}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c \\ 
    c'_1 \times c'_2 \; \rightarrow \; c'}
    {(\textbf{if} \; b_1 \; \textbf{then} \; c_1 \; \textbf{else} \; c'_1) \times (\textbf{if} \; b_2 \; \textbf{then} \; c_2 \; \textbf{else} \; c'_2) \; \rightarrow \;
    \textbf{assert}(b_1 \Leftrightarrow b_2); \; \textbf{if} \; b_1 \; \textbf{then} \; c \; \textbf{else} \; c'}

  \inferrule*[]
    {c_1 \times c \; \rightarrow \; c'_1 \\ 
    c_2 \times c \; \rightarrow \; c'_2}
    {(\textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2) \times c \; \rightarrow \;
    \textbf{if} \; b \; \textbf{then} \; c'_1 \; \textbf{else} \; c'_2}  
    
  \end{mathpar}
  \caption{Product construction rules.}
  \label{fig:product_construction_equal_struct}
\end{figure}

The figure above describes a set of rules to derive a product construction judgment of the type \emph{c$_1$ $\times$ c$_2$ $\rightarrow$ c.}
To make sure that \emph{c} simulates with precision the behaviour of \emph{c$_1$ and c$_2$}, the construction of products introduces \textbf{assert} statements.
During the phase of the verification of the program, these validation constraints are viewed as local assertions and discharged.
Let us consider the rule that synchronizes two loops, for example: \textbf{assert}(b$_1$ $\Leftrightarrow$ b$_2$) is necessary to guarantee that the number of iterations of each loop is the same.
This achieves what we aimed for, that is, the product program can be verified with standard logic.
\bigskip

\textbf{Proposition 1 -}  \emph{For all statements $c_1$ and $c_2$, pre-condition $\phi$ and post-condition $\psi$, if $c_1$ $\times$ $c_2$ $\rightarrow$ c and $\vDash$ {$\phi$} c {$\psi$} then $\vDash$ {$\phi$} $c_1$ \textasciitilde $c_2$ {$\psi$}.}

\bigskip
In other words, if \emph{c} is the resulting product program of \emph{c1} and \emph{c2}, then we can reason about the validity of the relational judgment between \emph{c1} and \emph{c2} through the validity of the standard judgment of \emph{c}.

As we mentioned before, the rules present in the previous figure are limited to structurally equivalent programs.
For example, if we consider two programs each with a loop and their guards are not equivalent, the product would have to be sequentially composed.
The structural translations proposed extend the construction of the already defined products in the form of a refinement relation, with a judgment of the form \emph{c} $\succcurlyeq$ \emph{c'}. 
This means that every execution of \emph{c} is an execution of \emph{c'} except when the latter's execution does not terminate:
\bigskip

\textbf{Definition 3 -}  \emph{A command c' is a refinement of c, if, for all states $\sigma$, $\sigma$':}    
\begin{itemize}
  \item{\emph{1. if $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$' then $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$'}}
  \vspace{-10pt}
  \item{\emph{2. if $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' then either the execution of c' with initial state $\sigma$ gets stuck, or $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$'.}}
\end{itemize}

\bigskip
In the figure below is described the set of rules that define judgments of the form $\vdash$ \emph{c} $\succcurlyeq$ \emph{c'}.

\begin{figure}[h]
  \centering
  \begin{mathpar}

  \inferrule*[]
    { }
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{assert}(b); \; c_1}
    
  \inferrule*[]
    { }
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{assert}(\neg b); \; c_2}

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{assert}(b); \; c; \; \textbf{while} \; b \; \textbf{do} \; c}

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{while} \; b \; \land \; b' \; \textbf{do} \; c}  

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{assert}(b); \; c; \; \textbf{assert}(\neg b)}

  \inferrule*[]
    {\vdash c \; \succcurlyeq \; c'}
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \vdash \textbf{while} \; b \; \textbf{do} \; c'}

  \inferrule*[]
    {\vdash c_1 \; \succcurlyeq \; c'_1 \\ \vdash c_2 \; \succcurlyeq \; c'_2}
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{if} \; b \; \textbf{then} \; c'_1 \; \textbf{else} \; c'_2}

  \inferrule*[]
    {\vdash c \; \succcurlyeq \; c' \\ \vdash c' \; \succcurlyeq \; c''}
    {\vdash c \; \succcurlyeq \; c''}

  \inferrule*[]
    { }
    {\vdash c \; \succcurlyeq \; c}

  \inferrule*[]
    {\vdash c_1 \; \succcurlyeq \; c'_1 \\ \vdash c_2 \; \succcurlyeq \; c'_2}
    {\vdash c_1 ; c_2 \; \succcurlyeq \; c'_1 ; c'_2}
    
  \end{mathpar}
  \caption{Syntactic reduction rules.}
  \label{fig:product_construction_reduction}
\end{figure}

One can see that the rules above point that the executions of \emph{c} and \emph{c'} match for every initial state that makes the introduced \textbf{assert} statements valid.
It is possible to prove that the judgment \emph{c} $\succcurlyeq$ \emph{c'} maintains a refinement relation by showing that, for every assertion $\phi$, if c' is $\phi$-nonstuck, then for all $\sigma$ $\in$ $\llbracket$$\phi$$\rrbracket$ such that $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' we have $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$'.
We add one more rule that makes a preliminary refinement transformation over the product components:


\begin{figure}[H]
  \centering
  \begin{mathpar}

  \inferrule*[]
    {\vdash c_1 \; \succcurlyeq \; c'_1 \\
     \vdash c_2 \; \succcurlyeq \; c'_2 \\
     c'_1 \times c'_2 \; \rightarrow \; c}
    {c_1 \times c_2 \; \rightarrow \; c}
    
  \end{mathpar}
\end{figure}

TODOOOOOO ADJUST THIS! this has to come after the figure above!
This does not invalidate \textbf{Proposition 1}. 
Finally, we formally reduce the problem of proving the validity of a relational judgment into the construction of the product program followed by the standard verification of that same product program.
\bigskip

\textbf{Proposition 2 -}  \emph{For all statements $c_1$ and $c_2$, pre-condition $\phi$ and post-condition $\psi$, if $c_1$ $\times$ $c_2$ $\rightarrow$ c and $\vdash$ {$\phi$} c {$\psi$} then $\vDash$ {$\phi$} $c_1$ \textasciitilde $c_2$ {$\psi$}.}

\bigskip


\subsection{Examples} 
\label{subsec:product_programs_examples}

Although product programs can be applied to several relational properties, such as non-interference and continuity, we will focus our examples on the correctness of program optimizations.

\subsubsection{Loop alignment} 
\label{subsubsec:product_programs_loop_alignment}

Loop alignment is an optimization that consists in improving cache effectiveness by increasing the proximity of the memory locations acessed in each iteration of the loop.
\bigskip

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Source program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 1@; 
        while (@i (*$\textcolor{red}{\leq}$*) N@) do
          @b[i] := a[i]@;
          @d[i] := b[i-1]@;
          @i++@
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Optimized program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        .j := 1.;
        .(*$\textcolor{blue}{\overline{d}}$*)[1] := (*$\textcolor{blue}{\overline{b}}$*)[0].;
        while (.j (*$\textcolor{blue}{\leq}$*) N-1.) do
          .(*$\textcolor{blue}{\overline{b}}$*)[j] := (*$\textcolor{blue}{\overline{a}}$*)[j].;
          .(*$\textcolor{blue}{\overline{d}}$*)[j+1] := (*$\textcolor{blue}{\overline{b}}$*)[j].;
          .j++.;
        .(*$\textcolor{blue}{\overline{b}}$*)[N] := (*$\textcolor{blue}{\overline{a}}$*)[N].
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.9\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Product program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        {a = (*$\overline{a}$*) (*$\land$*) b[0] = (*$\overline{b}$*)[0]}
          @i := 1@; 
          .j := 1.;
          assert(@i (*$\textcolor{red}{\leq}$*) N@);
          @b[i] := a[i]@; @d[i] := b[i-1]@; @i++@;
          .(*$\textcolor{blue}{\overline{d}}$*)[1] := (*$\textcolor{blue}{\overline{b}}$*)[0].;
          assert(@i (*$\textcolor{red}{\leq}$*) N@ (*$\Leftrightarrow$*) .j (*$\textcolor{blue}{\leq}$*) N-1.);
          while (@i (*$\textcolor{red}{\leq}$*) N@) do
            @b[i] := a[i]@; @d[i] := b[i-1]@; @i++@;
            .(*$\textcolor{blue}{\overline{b}}$*)[j] := (*$\textcolor{blue}{\overline{a}}$*)[j].; .(*$\textcolor{blue}{\overline{d}}$*)[j+1] := (*$\textcolor{blue}{\overline{b}}$*)[j].; .j++.;
            assert(@i (*$\textcolor{red}{\leq}$*) N@ (*$\Leftrightarrow$*) .j (*$\textcolor{blue}{\leq}$*) N-1.);
          .(*$\textcolor{blue}{\overline{b}}$*)[N] := (*$\textcolor{blue}{\overline{a}}$*)[N].
        {d[1,N] = (*$\overline{d}$*)[1,N]}
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \caption{Loop alignment.}
  \label{fig:loop_alignment}
\end{figure}

\bigskip
Consider \emph{N} $\geq$ 1.
In the source program above, note that, in each iteration, the array \emph{b} is accessed twice; firstly in the write to index \emph{i} and secondly in the read of position \emph{i-1}.
What loop alignment does is transform the acess of different indexes of \emph{b} into only one index, in this case, \emph{i}.
The corresponding product program starts by ensuring that the number of iterations of the loop is the same for the source and optimized programs, through the assignment statement \emph{$\overline{d}$[1] := $\overline{b}$[0]}.
To be verified, the program product needs a pre-condition, a post-condition and a loop invariant.
The pre-condition is \emph{a = $\overline{a}$ $\land$ b[0] := $\overline{b}$[0]}.
The post-condition is \emph{d[1, N] = $\overline{d}$[1, N]}, meaning that, for the indexes in the interval [1, \emph{N}], the values of the arrays \emph{d} and \emph{$\overline{d}$} are the same.
A suitable loop invariant is \emph{d[1, i) = $\overline{d}$[1, i)  $\land$  b[j] = a[j]  $\land$  $\overline{b}$[i] = b[i]  $\land$  i = j + 1}.
This specification guarantees that, if the input arrays \emph{a} and \emph{b} are equal, the values present in the array \emph{d} are the same when the execution of both product components terminates.


\subsubsection{Induction variable strength reduction} 
\label{subsubsec:product_programs_strength_reduction}

Product programs allow the verification of optimizations that \emph{maintain} the control flow of programs, but simply modify the basic blocks.
Some rather common examples of this are constant propagation and common subexpression elimination.
\bigskip

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Source program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 0@; 
        while (@i < N@) do
          @j := i * B + C@;
          @x += j@;
          @i++@
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Optimized program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        .i(*\textcolor{blue}{\textasciiacute}*) := 0.;
        .j(*\textcolor{blue}{\textasciiacute}*) := C.;
        while (.i(*\textcolor{blue}{\textasciiacute}*) < N.) do
          .x(*\textcolor{blue}{\textasciiacute}*) += j(*\textcolor{blue}{\textasciiacute}*).;
          .j(*\textcolor{blue}{\textasciiacute}*) += B.;
          .i(*\textcolor{blue}{\textasciiacute}*)++.
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.9\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Product program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 0@; .i(*\textcolor{blue}{\textasciiacute}*) := 0.; .j(*\textcolor{blue}{\textasciiacute}*) := C.;
        while (@i < N@ (*$\land$*) .i(*\textcolor{blue}{\textasciiacute}*) < N.) do
          @j := i * B + C@; @x += j@; @i++@
          .x(*\textcolor{blue}{\textasciiacute}*) += j(*\textcolor{blue}{\textasciiacute}*).; .j(*\textcolor{blue}{\textasciiacute}*) += B.; .i(*\textcolor{blue}{\textasciiacute}*)++.
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \caption{Induction variable strength reduction.}
  \label{fig:induction_var_strength_red}
\end{figure}

\bigskip
The figure above demonstrates the effects that applying strength reduction has in a small program.
In the example, \emph{j} is a derived induction variable defined as a linear function on the induction variable \emph{i}.
The optimization in question substitutes the statement \emph{j := i * B + C} by the equivalent and more performant statement \emph{j' += B} (multiplication is more costly than addition to a computer), makes the assignment \emph{x' += j'} come first inside the loop and adds an initial assignment \emph{j' = C} before the start of the loop.
To verify the correctness of the optimization, we need to guarantee that \emph{x = x'} is an invariant of the product program.
And to do that, we need the linear condition \emph{j := i * B + C} to be part of the loop invariant.


\section{Program Equivalence}
\label{sec:program_equivalence}

Program equivalence stands out as a crucial subject within the field of formal verification~\cite{DBLP:journals/fmsd/Strichman18}.
"Traditional" program correctness by itself is usually harder to prove than program equivalence, especially when it comes to real world programs, making the latter a topic whose research can reach success easier.

This work ~\cite{DBLP:journals/fmsd/KlebanovRU18} brings to light an interesting approach called regression verification, which combines regression testing with formal verification.
It is based on invariant inference techniques to automatically prove that two imperative pointer programs are equivalent, in the context of one being an updated version of the other.
By updated version we mean that the program has now more features, has been refactored or went through performance optimizations.

This automatic proof method is achieved by first transforming the two programs into Horn clauses over uninterpreted predicate symbols. 
Then, these clauses constrain equivalence witnessing coupling predicates that establish the relation of the two programs at key points.
Finally, if there is a solution for the coupling predicates, a Horn constraint solver is utilized to find it.
It may be important to mention that, according to the authors, the approach was implemented and its effectiveness demonstrated.

Besides the fact that the focus of our work does not include pointer programs, it is important to note that, due to the lack of human-written specification, this approach has its limitations.
The range of situations where regression verification can be applied is still confined to some programming language constructs and there are issues regarding the scalability.
