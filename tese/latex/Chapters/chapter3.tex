%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter3.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother


\chapter{State of the Art}
\label{cha:state_of_the_art}

\textbf{Definition: Structural equivalence -} we say that two programs are structurally equivalent if they present the same sequence of statements/commands.
For example, a program \emph{$P_1$} that consists in two assignments and a while loop is not equivalent to another program program \emph{$P_2$} if the latter's structure is a single for loop.
On the other hand, if \emph{$P_1$} and \emph{$P_2$} both contain, let us say, three assignments followed by a for loop (with a single assignment inside) that executes the same number of iterations, they are structurally equivalent, despite the values being assigned to the variables.

Note that this definition is useful when reading about self-composition and cross-products below in this capter.
However, we aim to allow verification of programs that would not be considered by this definition of structural equivalence and therefore be more flexible, but we still need some degree of similarity.
That is why we do not target programs that include exceptions.


\section{Self-composition} 
\label{sec:self_composition}

\subsection{Introduction}
\label{subsec:self_composition_intro}

The proposal of self-composition ~\cite{DBLP:conf/csfw/BartheDR04} is to offer an extensible and flexible way of controlling information flow.
This is usually done by information flow type systems, but they suffer from the issues that self-composition attempted to solve.
In the work mentioned above, the authors tried to address the static enforcement of information flow policies but focused on a specific one called non-interference.
Non-interference separates the state of a program into a public and a secret part and, by observing its execution, determines if there was a information leak on the secret part of the state.

Despite the well-defined focus, the efforts of the work specified before apply to several programming languages and different definitions of security, even including declassification, partially.
In the end, the authors were able to establish a general theory of self-composition to prove that programs are non-interfering.
One of the main features of self-composition is its expressiveness and that there is no need to prove the soundness of type systems, since it is based on logic.


\subsection{Exploring non-interference}
\label{subsec:exploring_non_interference}

In order to give an example of self-composition in the next subsection, we present now a simple program that respects the properties of termination-insensitivity and non-interference.

Consider a simple, deterministic and imperative language that allows sequential composition and the evaluation relation \(\langle P, u\rangle \Downarrow v\), where \(P\) is a program and \(u,v\) are memories.
Memory in this context means a map where the keys are the program variables of \(P\) and the values are the values of those program variables.
Furthermore, consider that all variables of \(P\) need to be either public or private, let \(\overrightarrow{x}\) be the set of all public variables in \(P\) and \(\overrightarrow{y}\) the set of all of its private variables.
For all memories \(u_1,u_2,v_1,v_2\), the properties of termination-insensitivity and non-interference for \(P\) can be described as:

\[ [\langle P, u_1\rangle \Downarrow v_1 \, \land \, \langle P, u_2\rangle \Downarrow v_2 \, \land \, u_1 {=}_{L} u_2] \Rightarrow v_1 {=}_{L} v_2 \]

where ${=}_{L}$ is the point-wise extension of equality on values to public parts of memories.

Consider [\(\overrightarrow{x'}, \overrightarrow{y'} / \overrightarrow{x}, \overrightarrow{y}\)] as being a renaming of the program variables \(\overrightarrow{x}\) and \(\overrightarrow{y}\) of \(P\) with fresh variables \(\overrightarrow{x'}\), \(\overrightarrow{y'}\) and let \(P'\) be as \(P\) but with its variables renamed: \(P\)[\(\overrightarrow{x'}, \overrightarrow{y'} / \overrightarrow{x}, \overrightarrow{y}\)].
Also, to refer to the the disjoint union of two memories we will use the $\uplus$ symbol.
Therefore, we have \(\langle P, u\rangle \Downarrow v \, \land \, \langle P', u'\rangle \Downarrow v' \) iff \(\langle P; P, u \uplus u' \rangle \Downarrow v \uplus v'\).
We can now rearrange non-interference, for memories \(u,u',v,v'\), as the following:

\[ (\langle P; P, u \uplus u' \rangle \Downarrow v \uplus v' \, \land \, u = {}_{\overrightarrow{x}} u' \circ [\overrightarrow{x} / \overrightarrow{x'}]) \Rightarrow v = {}_{\overrightarrow{x}} v' \circ [\overrightarrow{x} / \overrightarrow{x'}] \]

where $\circ$ is the symbol for function composition and \(= {}_{\overrightarrow{x}}\) is the point-wise extension of equality on values to the restriction of memories to $\overrightarrow{x}$.
With this formulation, we are able to reduce the non-interference property of \(P\) to a property of all the executions of \(P; P'\).
Therefore, an alternative characterization of non-interference can be presented by using programming logics, since they are sound and relatively complete with relation to the operational semantic.
We can describe non-interference using Hoare triples:

\[ \{\overrightarrow{x} = \overrightarrow{x'}\} \, P; P' \, \{\overrightarrow{x} = \overrightarrow{x'}\} \]


\subsection{Code Example}
\label{subsec:self_composition_example}

Consider the following program:

\[ x := y; \, x := 0 \]

and \(x \mapsto x'\) and \(y \mapsto y'\) as the renaming functions. 
The program is non-interferent iff

\[ \{x = x'\} \, x := y; x := 0 \, x' := y'; x' := 0 \, \{x = x'\} \]

We are able to characterize information flow policies, which include some types of declassification, through the replacement of the =- relation by other (partial) equivalence relation.
More generally, this form of characterization allows us to use existing verification tools to prove (or disprove) information flow policies for a given program.


\FloatBarrier
\section{Cross-products} 
\label{sec:cross_products}

\subsection{Introduction} 
\label{subsec:cross_products_intro}

Cross-products~\cite{DBLP:conf/fm/ZaksP08} aim to prove program equivalence, with a strong focus on verifying compiler optimizations.
Instead of proving that the two input programs are equivalent, the analysis is done by combining them into one: the cross-product.
With this, instead of recurring to program analysis and proof rules developed specifically for translation validation, it became possible to use existing methods and tools to prove properties of a single program. 
Despite handling the most common intraprocedural compiler optimizations, cross-products can not be applied to two input programs with dissimilar structures, which is a major constraint of this work.

One important aspect of CoVaC, a framework developed by the authors of the aforementioned article, is that it was made to validate program equivalence in general while balancing precision and efficiency.
The approach was to divide the analysis in two phases, a faster one first and a more precise after that.
The first phase utilizes a fast, yet imprecise value numbering algorithm whose results are used to determine if it is necessary to apply the second phase. 
This final part is based on assertion checking, a static program verification method which, under the hood, computes the weakest-precondition~\cite{DBLP:books/ph/Dijkstra76} using CVC3~\cite{cvc3}. 

The results presented on the cited work reveal that CoVaC was able to verify a very considerable set of optimizations of LLVM~\cite{llvm}, a complex modern compiler.
Yet, the tool does not support verification of interprocedural optimizations or information flow policies, for example, but these are limitations that the authors showed interest in addressing.


\subsection{A practical example to simplify the theory} 
\label{subsec:cross_products_ex}

Consider the following example, composed by a source program and its optimized version:

\begin{tabbing}
  \hspace{2cm}\= \textbf{Source Program (S)} \hspace{2cm} \= \textbf{Optimized Program (T)} \\ 
  \> \emph{entry} \> \emph{entry} \\
  \> \emph{  read x} \> \emph{  read x} \\
  \> \emph{  y := x + 0} \> \emph{  z := x} \\
  \> \emph{  z := y * 1} \> \emph{  write z} \\
  \> \emph{  write z} \> \emph{exit} \\
  \> \emph{exit} \> \emph{} 
  \label{tabbing:cross_products_example_code}
\end{tabbing}

This example is small but enough to demonstrate the idea of cross-products in practice.
The transformation removes trivial arithmetic that does not change the final result and represents, actually, two optimizations: constant folding (\emph{y := x + 0; z := y * 1} becomes \emph{y := x; z := y}) followed by dead code elimination (\emph{y := x; z := y} becomes \emph{z := x}).

Let us now informally model each program as a small transition graph with nodes (locations) and labeled edges.
We mark $read$ and $write$ as observable transitions, while assigns are internal but still visible to the cross-product as assignment labels.
Consider the source program's ($S$) nodes: $S_0$ = entry, $S_1$ = after read, $S_2$ = after \emph{y := x + 0}, $S_3$ = after \emph{z := y * 1}, $S_4$ = exit.
Therefore, the edges of $S$ can be described as:
\begin{tabbing}
  \emph{($S_0$ $\rightarrow$ $S_1$): read(x)} \\
  \emph{($S_1$ $\rightarrow$ $S_2$): y := x + 0} \\
  \emph{($S_2$ $\rightarrow$ $S_3$): z := y * 1} \\
  \emph{($S_3$ $\rightarrow$ $S_4$): write(z)}
  \label{tabbing:cross_products_transitions_s}
\end{tabbing}

Consider now the optimized program's ($T$) nodes: $T_0$ = entry, $T_1$ = after read, $T_2$ = after \emph{z := x}, $T_3$ = before write, $T_4$ = exit.
$T$'s edges are the following:
\begin{tabbing}
  \emph{($T_0$ $\rightarrow$ $T_1$): read(x)} \\
  \emph{($T_1$ $\rightarrow$ $T_2$): z := x} \\
  \emph{($T_2$ $\rightarrow$ $T_3$): $\epsilon$} \\
  \emph{($T_3$ $\rightarrow$ $T_4$): write(z)}
  \label{tabbing:cross_products_example_transitions_t}
\end{tabbing}

The $\epsilon$ steps are inserted as needed to align executions, as one can see in the transition between $T_2$ and $T_3$.
Next, we construct the comparison graph \emph{C = S $\boxtimes$ T}.
The nodes of $C$ are pairs of the type $<S_i, T_j>$ that start at $<S_0, T_0>$.
There are 3 types of edge pairing: we match reads with reads, writes with writes (and then check the written values) and assignments with assignments.
Regarding assignments, we only match them when both sides are at corresponding cutpoints (a given node in the control-flow graph), otherwise we allow an assignment to pair with an $\epsilon$ on the other side, making it wait.
Therefore, the constructed comparison graph C contains paths that execute the two programs in lockstep, allowing $\epsilon$-padding where needed.
We will now showcase this using the example.

When we start, at $<S_0, T_0>$, we see that both programs have the same instruction, $read(x)$, so we add an edge labeled $(read(x); read(x))$ to $<S_1, T_1>$.
At $<S_1, T_1>$, the next instruction of the source program is \emph{y := x + 0}, while in the optimized program the next command is \emph{z := x}.
Since they are both assignments, we simply compose them as in the previous edge, resulting in the addition of an edge with the label \emph{(y := x+0 ; z := x)} to $<S_2, T_2>$.
Note that even though the expressions differ syntactically, the edge-matching logic in the compose algorithm allows pairing assignment edges.
Later, the invariant generator / solver will establish if they compute the same value or not.
From $<S_2, T_2>$, we get another assignment from the source program \emph{z := y * 1} but an $\epsilon$ from the optimized program.
According to rules, we can pair the source assignment with an $\epsilon$ on the target, making it wait, so we add an edge labeled \emph{(z := y * 1; $\epsilon$)} to $<S_3, T_3>$.
Finally, from $<S_3, T_3>$ there is a \emph{write(z)} in both programs, which means we add the last edge, with the label \emph{(write(z); write(z))}, to the exit pair $<S_4, T_4>$.

We will now discuss the invariant network.
Consider $\varphi_n$ an assertion associated with a node $n$, an assertion network $\phi_C = \{ \varphi_n : n \in$ locations of $C \}$ and $d$ a data state.
For a program $C$, we say that an assertion network is $invariant$ if, for every state $<n;\overrightarrow{d}>$ ocurring in a computation, $d \vDash \varphi_n$.
This means that $d$ satisfies the corresponding assertion $\varphi_n$ on every visit of a computation of node $n$. 
The key cutpoints are after reads and before writes.

Consider $x_s$ the notation for the variable $x$ in the source program and $x_t$ in the optimized version.
We now describe the invariants for the example we have been using.
After the $read(x)$, we reach $\phi<S_1, T_1>$, where we say $x_s = x_t$.
Next, after the first assignment on each side, $\phi<S_2, T_2>$, we want to assert that the variable that will become $z$ has the same value in both programs.
After composing the actual assignments, we can infer the following.
From $S$ we get $y_s := x_s + 0$, which can be simplified to $y_s := x_s$.
Next, also from $S$: $z_s := y_s * 1$, we get $z_s := y_s$, and since $y_s := x_s$, we get $z_s := x_s$.
We also get $z_t := x_t$ after we go from $T_1$ to $T_2$, therefore, at $<S_3, T_3>$, we can assert $z_s := z_t$ before the write.
Inductively, we can express $\phi<S_3, T_3> : \, z_s := z_t \, \land$ \emph{(other observational equalities)}.
This is exactly the kind of invariant network the CoVaC framework expects, and the authors also suggest a number of invariant generation techniques that can be used to compute these assertions automatically.

One of the most important phases of CoVaC is the checking of the witness verification condition.
At every write node, the tool produces a simple verification condition: the invariant at that node must imply that the written values are equal.
In our example, at the paired write node $<S_3, T_3>$ the witness VC is ${\varphi}_{<S_3, T_3>} \rightarrow (z_s = z_t)$.
If the invariant network proves $z_s = z_t$ at that node, the write outputs are equal; since reads/writes are the only observable I/O, this shows stuttering-equivalent observable behavior.
According to the main theorem in the aforementioned work, the existence of such witness comparison graph and valid witness VCs implies that the target is a correct translation of the source.

Intuitively, this approach proves correctness for the three following reasons.
Firstly, the comparison graph represents \emph{all aligned executions} of source and target, with $\epsilon$ padding to allow one side to do assignments while the other waits.
Furthermore, the invariants guarantee that at observable points (reads, writes, and procedure boundaries) the observable variables agree.
Finally, the witness VC at writes enforces equality of output values.
Together, these satisfy cross-product's definition of a witness, therefore the translation is correct.


\FloatBarrier
\section{Product Programs} 
\label{sec:product_programs}

\subsection{Motivation} 
\label{subsec:product_programs_motivation}

Relational Hoare Logic serves as a good starting point to compare the behavior of two different executions of the same program or even two different programs.
However, there are few available tools and practical reasoning logics for relational verification.
One of the main limitations of the existing ones~\cite{DBLP:conf/popl/Benton04, DBLP:journals/tcs/Yang07} is the constraint of \emph{structural equivalence}.
Inversely, traditional program verification has diverse and extensive tool support.
Therefore, as a way of getting around that limitation, we can take advatange of the existing tool support if we are able to soundly reduce relational verification tasks into standard verification ones.
This means we would translate Hoare quadruples (\{$\Phi$\} \emph{$P_1$} \emph{\textasciitilde} \emph{$P_2$} \{$\Psi$\}) into Hoare triples (\{$\phi$\} \emph{P} \{$\psi$\}), making sure that: if the original quadruple is valid, then the generated triple is also valid; if the if the original quadruple is not valid, the generated triple would also be invalid.
Considering $\vDash$ the symbol for validity, the objective is finding \emph{$\phi$, P, $\psi$} that:
\[ \vDash \{\Phi\} \, \textbf{$P_1$} \sim \textbf{$P_2$} \, \{\Psi\} \ \ \ \rightarrow \ \ \ \ \ \vDash \{\phi\} \, \textbf{P} \, \{\psi\} \]

Let us consider two imperative and \emph{separable} programs \emph{$P_1$} and \emph{$P_2$}.
This enables the capacity of the assertions to be seen as first-order formulas about the variables of \emph{$P_1$} and \emph{$P_2$}.
Self-composition~\cite{DBLP:conf/csfw/BartheDR04} represents a way of establishing the wanted equalities mentioned above: \emph{P $\equiv$ $P_1$;$P_2$}, \emph{$\phi$ $\equiv$ $\Phi$} and \emph{$\psi$ $\equiv$ $\Psi$}.
Despite being characterized by soundness and relative completeness~\cite{DBLP:journals/jlp/BartheCK16}, this construction is impractical for the authors of this~\cite{DBLP:conf/sas/TerauchiA05} work.
One of the two main reasons is that the existing automatic safety analysis tools available are not powerful enough to verify most of realistic problems and, when they can, there is a lack of performance.
The other reason is that the safety analysis developed for naturally 1-safety problems is not expected to advance significantly in the foreseeable future.

There is another relevant method previously discussed in this \hyperref[sec:cross_products]{this} section, the cross-products.
These suffer from the constraint of structural equivalence, making impossible to reason about properties or program optimizations that are based on different control flows.

Product Programs~\cite{DBLP:conf/fm/BartheCK11} appear to be the best path to follow, since they represent a general notion that combines the flexibility of self-composition in executing asynchronous steps and the efficiency of cross-products when it comes to treat synchronous steps. 
Product programs are a way to reduce relational verification into standard verification through the construction of a product program \emph{P} that simulates the execution statements of any two programs \emph{$P_1$} and \emph{$P_2$}.


\FloatBarrier
\subsection{From relational verification to standard verification} 
\label{subsec:product_programs_relverif_to_stdverif}

\subsubsection{A first intuition} 
\label{subsubsec:product_programs_intuition}

As a way to provide a first intuition of construction of a product from structurally dissimilar programs before we dive into the definitions and rules later in this subsection, consider the simple \hyperref[fig:pp_first_intuition]{example} below (assume 0 $\leq$ N).

\begin{figure}[h]
  \centering

  \begin{subfigure}[t]{0.28\textwidth}
    \centering
    \makebox[\linewidth][l]{\textbf{Source program}}\par\smallskip
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, linewidth=\linewidth, xleftmargin=0pt, framexleftmargin=0pt]
@i := 1@; 
while (@i (*$\textcolor{red}{\leq}$*) N@) do
  @x += i@;
  @i++@
    \end{lstlisting}
  \end{subfigure}\hfill%
  \begin{subfigure}[t]{0.28\textwidth}
    \centering
    \makebox[\linewidth][l]{\textbf{Transformed program}}\par\smallskip
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, linewidth=\linewidth, xleftmargin=0pt, framexleftmargin=0pt]
çj := 1ç; 
while (çj (*$\textcolor{blue}{\leq}$*) Nç) do
  çy += jç;
  çj++ç
    \end{lstlisting}
  \end{subfigure}\hfill%
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \makebox[\linewidth][l]{\textbf{Product program}}\par\smallskip
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, linewidth=\linewidth, xleftmargin=0pt, framexleftmargin=0pt]
@i := 1@; @x += i@; @i++@; çj := 1ç;
while (@i (*$\textcolor{red}{\leq}$*) N@) do
  @x += i@; @i++@;
  çy += jç; çj++ç
    \end{lstlisting}
  \end{subfigure}

  \caption{First intuition source, transformed and product programs.}
  \label{fig:pp_first_intuition}
\end{figure}

The first step to build the product program is to unroll the first loop iteration of the source code before synchronizing the loop statements.
Rather than relying plainly on self-composition, this idea maximizes synchronization and differs from a functional interpretation of the product components.
Furthermore, this approach also reduces considerably the complexity of the invariants required to prove the product program.
In the case of a sequential composition of the source and transformed programs, we would need to provide invariants of the form \emph{x = X + $\frac{i(i-1)}{2}$} and \emph{y = Y + $\frac{j(j-1)}{2}$}, respectively, assuming the pre-conditions \emph{x = X} and \emph{y = Y}.
On the other hand, if we construct the product program, the loop invariant \emph{i = j $\land$ x = y} is enough to verify that the two first programs above satisfy the pre and post-relation \emph{x = y}.


\FloatBarrier
\subsubsection{Establishing the ground rules} 
\label{subsubsec:product_programs_ground_rules}

The commands in our programming model will stick to the grammar defined previously \hyperref[sub:relational_hoare_logic_formal_proof_rules]{here}, but we redefine some of its components in a more specific way.

\emph{x} ranges over a set of integer variables V$_i$, \emph{a} ranges over a set of array variables V$_a$, let V denote V$_i$ $\cup$ V$_a$, and assuming V$_i$ $\cap$ V$_a$ = $\emptyset$.
Let \emph{e} $\in$ AExp and \emph{b} $\in$ BExp range over integer and boolean expressions.
Their semantics are given by ${(\llbracket e \rrbracket)}_{e \in AExp}$ : S $\rightarrow$ $\mathbb{Z}$ and ${(\llbracket b \rrbracket)}_{b \in BExp}$ : S $\rightarrow$ $\mathbb{B}$, respectively.
Execution states' representation is \emph{S = (V$_i$ $\rightharpoonup$ $\mathbb{Z}$) X (V$_a$ $\rightharpoonup$ ($\mathbb{Z}$ $\rightharpoonup$ $\mathbb{Z}$)).}
The semantics of the commands are standard, deterministic and are based on the relation $\langle$c, $\sigma$$\rangle$ $\rightsquigarrow$ $\langle$c', $\sigma$'$\rangle$.

Notice that $\langle$\textbf{skip}, $\sigma$$\rangle$ marks the end of the programs, \textbf{assert}(b) blocks the execution of the program if \emph{b} is false, and we let $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' mean $\langle$c, $\sigma$$\rangle$ $\rightsquigarrow$* $\langle$\textbf{skip}, $\sigma$'$\rangle$.
An assertion $\phi$ is a first-order formula with variables in V.
We let $\llbracket\phi\rrbracket$ denote the set of states satisfying $\phi$.
Finally, we let var(\emph{c}) $\subseteq$ V and var($\phi$) $\subseteq$ V denote the set of (free) variables of a command c and assertion $\phi$, respectively.

As we discussed before in this document, two commands are \emph{separable} if they operate on disjoint sets of variables.
Similarly, we consider that two states are \emph{separable} if their domains are disjoint.
Consider $\mu$$_1$ $\uplus$ $\mu$$_2$ represent the union of finite maps:
\[
(\mu_1 \uplus \mu_2) \ x =
\begin{cases}
    \mu_1 x & \text{if } x \in \operatorname{dom}(\mu_1) \\
    \mu_2 x & \text{if } x \in \operatorname{dom}(\mu_2)
\end{cases}
\]

and overload this notation for the union of separable states ($\mu$,$\nu$) $\uplus$ ($\mu$',$\nu$'), whose definition is ($\mu$ $\uplus$ $\mu$', $\nu$ $\uplus$ $\nu$').
Taking into account that we assume that the states are separable, another way of looking at assertions is by viewing them as relations on states: ($\sigma$$_1$, $\sigma$$_2$) $\in$ $\llbracket$$\phi$$\rrbracket$ iff $\sigma$$_1$ $\uplus$ $\sigma$$_2$ $\in$ $\llbracket$$\phi$$\rrbracket$.
Therefore, the definition below illustrates the formal statement of valid relational specifications.
\bigskip

\textbf{Definition 1 -}  \emph{Two commands $c_1$ and $c_2$ satisfy the pre-condition $\phi$ and the post-condition $\psi$ described by a valid Hoare quadruple if,
                                for all states $\sigma$$_1$, $\sigma$$_2$, $\sigma$$_1$', $\sigma$$_2$' such that $\sigma_1$ $\uplus$ $\sigma_2$ $\in$ $\llbracket$$\phi$$\rrbracket$
                                and $\langle$$c_1$, $\sigma$$_1$$\rangle$ $\Downarrow$ $\sigma$$_1$' and $\langle$$c_2$, $\sigma$$_2$$\rangle$ $\Downarrow$ $\sigma$$_2$', we have $\sigma_1$' $\uplus$ $\sigma_2$' $\in$ $\llbracket$$\psi$$\rrbracket$.}

\bigskip
Since our objective is to reduce the validity of Hoare quadruples to validity of Hoare triples, we must say that we establish our notion of valid Hoare triple as stronger than usual.
It requires that the command allows the program to finish its execution, i.e., the command is \emph{non-stuck}.
\bigskip

\textbf{Definition 2 -}  \emph{A Hoare triple {$\phi$} c {$\psi$} is valid ($\vDash$ \{$\phi$\} c \{$\psi$\}) if c is $\phi$-nonstuck and for all $\sigma$, $\sigma$' $\in$ S, $\sigma$ $\in$ $\llbracket$$\phi$$\rrbracket$ and $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' imply $\sigma$' $\in$ $\llbracket$$\psi$$\rrbracket$.}

\bigskip
This notion of validity requires, however, that we extend Hoare logic for it to be able to treat \textbf{assert} statements.
The necessary rule is:
\begin{figure}[h]
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \begin{mathpar}
        \inferrule*[] { } {\vdash \ \{ \textit{b} \land \Phi \} \ \textbf{assert}\text{(b)} \ \{ \Phi \}}
      \end{mathpar}
    \end{minipage}
  \end{center}
\end{figure}


\FloatBarrier
\subsubsection{Construction of Product Programs} 
\label{subsubsec:product_programs_construction}

Firstly in this section, we define the rules that will be used to deal with structurally equivalent programs. 
After that, we increment that set with structural transformations to allow the treatment of programs with different structures.

\begin{figure}[h]
  \centering
  \begin{mathpar}

  \inferrule*[]
    { }
    {c_1 \times c_2 \; \rightarrow \; c_1 ; c_2}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c \\ 
     c'_1 \times c'_2 \; \rightarrow \; c'}
    {(c_1 ; c'_1) \times (c_2 ; c'_2) \; \rightarrow \; c ; c'}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c}
    {(\textbf{while} \; b_1 \; \textbf{do} \; c_1) \times (\textbf{while} \; b_2 \; \textbf{do} \; c_2) \; \rightarrow \;
    \textbf{assert}(b_1 \Leftrightarrow b_2); \; \textbf{while} \; b_1 \; \textbf{do} \; (c ; \textbf{assert}(b_1 \Leftrightarrow b_2))}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c \\ 
    c'_1 \times c'_2 \; \rightarrow \; c'}
    {(\textbf{if} \; b_1 \; \textbf{then} \; c_1 \; \textbf{else} \; c'_1) \times (\textbf{if} \; b_2 \; \textbf{then} \; c_2 \; \textbf{else} \; c'_2) \; \rightarrow \;
    \textbf{assert}(b_1 \Leftrightarrow b_2); \; \textbf{if} \; b_1 \; \textbf{then} \; c \; \textbf{else} \; c'}

  \inferrule*[]
    {c_1 \times c \; \rightarrow \; c'_1 \\ 
    c_2 \times c \; \rightarrow \; c'_2}
    {(\textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2) \times c \; \rightarrow \;
    \textbf{if} \; b \; \textbf{then} \; c'_1 \; \textbf{else} \; c'_2}  
    
  \end{mathpar}
  \caption{Product construction rules.}
  \label{fig:product_construction_equal_struct}
\end{figure}

The \hyperref[fig:product_construction_equal_struct]{figure} describes a set of rules to derive a product construction judgment of the type \emph{c$_1$ $\times$ c$_2$ $\rightarrow$ c.}
To make sure that \emph{c} simulates with precision the behavior of \emph{c$_1$ and c$_2$}, the construction of products introduces \textbf{assert} statements.
During the phase of the verification of the program, these validation constraints are viewed as local assertions and discharged.
Let us consider the rule that synchronizes two loops, for example: \textbf{assert}(b$_1$ $\Leftrightarrow$ b$_2$) is necessary to guarantee that the number of iterations of each loop is the same.
This achieves what we aimed for, that is, the product program can be verified with standard logic.
\bigskip

\textbf{Proposition 1 -}  \emph{For all statements $c_1$ and $c_2$, pre-condition $\phi$ and post-condition $\psi$, if $c_1$ $\times$ $c_2$ $\rightarrow$ c and $\vDash$ {$\phi$} c {$\psi$} then $\vDash$ {$\phi$} $c_1$ \textasciitilde $c_2$ {$\psi$}.}

\bigskip
In other words, if \emph{c} is the resulting product program of \emph{c1} and \emph{c2}, then we can reason about the validity of the relational judgment between \emph{c1} and \emph{c2} through the validity of the standard judgment of \emph{c}.

As we mentioned before, the rules present in the previous \hyperref[fig:product_construction_equal_struct]{figure} are limited to structurally equivalent programs.
For example, if we consider two programs each with a loop and their guards are not equivalent, the product would have to be sequentially composed.
The structural translations proposed extend the construction of the already defined products in the form of a refinement relation, with a judgment of the form \emph{c} $\succcurlyeq$ \emph{c'}. 
This means that every execution of \emph{c} is an execution of \emph{c'} except when the latter's execution does not terminate:
\bigskip

\textbf{Definition 3 -}  \emph{A command c' is a refinement of c, if, for all states $\sigma$, $\sigma$':}    
\begin{itemize}
  \item{\emph{1. if $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$' then $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$'}}
  \vspace{-10pt}
  \item{\emph{2. if $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' then either the execution of c' with initial state $\sigma$ gets stuck, or $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$'.}}
\end{itemize}

\bigskip
In the \hyperref[fig:product_construction_reduction]{figure} is described the set of rules that define judgments of the form $\vdash$ \emph{c} $\succcurlyeq$ \emph{c'}.

\begin{figure}[h]
  \centering
  \begin{mathpar}

  \inferrule*[]
    { }
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{assert}(b); \; c_1}
    
  \inferrule*[]
    { }
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{assert}(\neg b); \; c_2}

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{assert}(b); \; c; \; \textbf{while} \; b \; \textbf{do} \; c}

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{while} \; b \; \land \; b' \; \textbf{do} \; c}  

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{assert}(b); \; c; \; \textbf{assert}(\neg b)}

  \inferrule*[]
    {\vdash c \; \succcurlyeq \; c'}
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \vdash \textbf{while} \; b \; \textbf{do} \; c'}

  \inferrule*[]
    {\vdash c_1 \; \succcurlyeq \; c'_1 \\ \vdash c_2 \; \succcurlyeq \; c'_2}
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{if} \; b \; \textbf{then} \; c'_1 \; \textbf{else} \; c'_2}

  \inferrule*[]
    {\vdash c \; \succcurlyeq \; c' \\ \vdash c' \; \succcurlyeq \; c''}
    {\vdash c \; \succcurlyeq \; c''}

  \inferrule*[]
    { }
    {\vdash c \; \succcurlyeq \; c}

  \inferrule*[]
    {\vdash c_1 \; \succcurlyeq \; c'_1 \\ \vdash c_2 \; \succcurlyeq \; c'_2}
    {\vdash c_1 ; c_2 \; \succcurlyeq \; c'_1 ; c'_2}
    
  \end{mathpar}
  \caption{Syntactic reduction rules.}
  \label{fig:product_construction_reduction}
\end{figure}

One can see that the \hyperref[fig:product_construction_reduction]{rules} point that the executions of \emph{c} and \emph{c'} match for every initial state that makes the introduced \textbf{assert} statements valid.
It is possible to prove that the judgment \emph{c} $\succcurlyeq$ \emph{c'} maintains a refinement relation by showing that, for every assertion $\phi$, if c' is $\phi$-nonstuck, then for all $\sigma$ $\in$ $\llbracket$$\phi$$\rrbracket$ such that $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' we have $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$'.
We add one more rule that makes a preliminary refinement transformation over the product components:

\begin{figure}[H]
  \begin{center}
  \begin{minipage}{\linewidth}
  \centering
    \begin{mathpar}
      \inferrule*[]
        {\vdash c_1 \; \succcurlyeq \; c'_1 \\
        \vdash c_2 \; \succcurlyeq \; c'_2 \\
        c'_1 \times c'_2 \; \rightarrow \; c}
        {c_1 \times c_2 \; \rightarrow \; c}
    \end{mathpar}
  \end{minipage}
  \end{center}
\end{figure}

This does not invalidate \textbf{Proposition 1}. 
Finally, we formally reduce the problem of proving the validity of a relational judgment into the construction of the product program followed by the standard verification of that same product program.
\bigskip

\textbf{Proposition 2 -}  \emph{For all statements $c_1$ and $c_2$, pre-condition $\phi$ and post-condition $\psi$, if $c_1$ $\times$ $c_2$ $\rightarrow$ c and $\vdash$ {$\phi$} c {$\psi$} then $\vDash$ {$\phi$} $c_1$ \textasciitilde $c_2$ {$\psi$}.}

\bigskip


\FloatBarrier
\subsection{Examples} 
\label{subsec:product_programs_examples}

Although product programs can be applied to several relational properties, such as non-interference and continuity, we will focus our examples on the correctness of program optimizations.

\subsubsection{Loop alignment} 
\label{subsubsec:product_programs_loop_alignment}

Loop alignment is an optimization that consists in improving cache effectiveness by increasing the proximity of the memory locations accessed in each iteration of the loop.
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Source program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 1@; 
        while (@i (*$\textcolor{red}{\leq}$*) N@) do
          @b[i] := a[i]@;
          @d[i] := b[i-1]@;
          @i++@
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Optimized program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        çj := 1ç;
        ç(*$\textcolor{blue}{\overline{d}}$*)[1] := (*$\textcolor{blue}{\overline{b}}$*)[0]ç;
        while (çj (*$\textcolor{blue}{\leq}$*) N-1ç) do
          ç(*$\textcolor{blue}{\overline{b}}$*)[j] := (*$\textcolor{blue}{\overline{a}}$*)[j]ç;
          ç(*$\textcolor{blue}{\overline{d}}$*)[j+1] := (*$\textcolor{blue}{\overline{b}}$*)[j]ç;
          çj++ç;
        ç(*$\textcolor{blue}{\overline{b}}$*)[N] := (*$\textcolor{blue}{\overline{a}}$*)[N]ç
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.9\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Product program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        {a = (*$\overline{a}$*) (*$\land$*) b[0] = (*$\overline{b}$*)[0]}
          @i := 1@; 
          çj := 1ç;
          assert(@i (*$\textcolor{red}{\leq}$*) N@);
          @b[i] := a[i]@; @d[i] := b[i-1]@; @i++@;
          ç(*$\textcolor{blue}{\overline{d}}$*)[1] := (*$\textcolor{blue}{\overline{b}}$*)[0]ç;
          assert(@i (*$\textcolor{red}{\leq}$*) N@ (*$\Leftrightarrow$*) çj (*$\textcolor{blue}{\leq}$*) N-1ç);
          while (@i (*$\textcolor{red}{\leq}$*) N@) do
            @b[i] := a[i]@; @d[i] := b[i-1]@; @i++@;
            ç(*$\textcolor{blue}{\overline{b}}$*)[j] := (*$\textcolor{blue}{\overline{a}}$*)[j]ç; ç(*$\textcolor{blue}{\overline{d}}$*)[j+1] := (*$\textcolor{blue}{\overline{b}}$*)[j]ç; çj++ç;
            assert(@i (*$\textcolor{red}{\leq}$*) N@ (*$\Leftrightarrow$*) çj (*$\textcolor{blue}{\leq}$*) N-1ç);
          ç(*$\textcolor{blue}{\overline{b}}$*)[N] := (*$\textcolor{blue}{\overline{a}}$*)[N]ç
        {d[1,N] = (*$\overline{d}$*)[1,N]}
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \caption{Loop alignment.}
  \label{fig:loop_alignment}
\end{figure}

Consider \emph{N} $\geq$ 1.
In the source \hyperref[fig:loop_alignment]{program}, note that, in each iteration, the array \emph{b} is accessed twice; firstly in the write to index \emph{i} and secondly in the read of position \emph{i-1}.
What loop alignment does is transform the access of different indexes of \emph{b} into only one index, in this case, \emph{i}.
The corresponding product program starts by ensuring that the number of iterations of the loop is the same for the source and optimized programs, through the assignment statement \emph{$\overline{d}$[1] := $\overline{b}$[0]}.
To be verified, the program product needs a pre-condition, a post-condition and a loop invariant.
The pre-condition is \emph{a = $\overline{a}$ $\land$ b[0] := $\overline{b}$[0]}.
The post-condition is \emph{d[1, N] = $\overline{d}$[1, N]}, meaning that, for the indexes in the interval [1, \emph{N}], the values of the arrays \emph{d} and \emph{$\overline{d}$} are the same.
A suitable loop invariant is \emph{d[1, i) = $\overline{d}$[1, i)  $\land$  b[j] = a[j]  $\land$  $\overline{b}$[i] = b[i]  $\land$  i = j + 1}.
This specification guarantees that, if the input arrays \emph{a} and \emph{b} are equal, the values present in the array \emph{d} are the same when the execution of both product components terminates.

\FloatBarrier
\subsubsection{Induction variable strength reduction} 
\label{subsubsec:product_programs_strength_reduction}

Product programs allow the verification of optimizations that \emph{maintain} the control flow of programs, but simply modify the basic blocks.
Some rather common examples of this are constant propagation and common subexpression elimination.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Source program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 0@; 
        while (@i < N@) do
          @j := i * B + C@;
          @x += j@;
          @i++@
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Optimized program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        çi(*\textcolor{blue}{\textasciiacute}*) := 0ç;
        çj(*\textcolor{blue}{\textasciiacute}*) := Cç;
        while (çi(*\textcolor{blue}{\textasciiacute}*) < Nç) do
          çx(*\textcolor{blue}{\textasciiacute}*) += j(*\textcolor{blue}{\textasciiacute}*)ç;
          çj(*\textcolor{blue}{\textasciiacute}*) += Bç;
          çi(*\textcolor{blue}{\textasciiacute}*)++ç
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.9\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Product program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 0@; çi(*\textcolor{blue}{\textasciiacute}*) := 0ç; çj(*\textcolor{blue}{\textasciiacute}*) := Cç;
        while (@i < N@ (*$\land$*) çi(*\textcolor{blue}{\textasciiacute}*) < Nç) do
          @j := i * B + C@; @x += j@; @i++@
          çx(*\textcolor{blue}{\textasciiacute}*) += j(*\textcolor{blue}{\textasciiacute}*)ç; çj(*\textcolor{blue}{\textasciiacute}*) += Bç; çi(*\textcolor{blue}{\textasciiacute}*)++ç
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \caption{Induction variable strength reduction.}
  \label{fig:induction_var_strength_red}
\end{figure}

The \hyperref[fig:induction_var_strength_red]{figure} demonstrates the effects that applying strength reduction has in a small program.
In the example, \emph{j} is a derived induction variable defined as a linear function on the induction variable \emph{i}.
The optimization in question substitutes the statement \emph{j := i * B + C} by the equivalent and more performant statement \emph{j' += B} (multiplication is more costly than addition to a computer), makes the assignment \emph{x' += j'} come first inside the loop and adds an initial assignment \emph{j' = C} before the start of the loop.
To verify the correctness of the optimization, we need to guarantee that \emph{x = x'} is an invariant of the product program.
And to do that, we need the linear condition \emph{j := i * B + C} to be part of the loop invariant.

\FloatBarrier
\section{WhyRel}
\label{sec:whyrel}

\subsection{Introduction}
\label{subsec:whyrel_intro}

WhyRel~\cite{whyrel} is a tool that aims to verify relational properties of pointer programs based on relational region logic, in an auto-active way.
It also supports state-based encapsulation and dynamic framing. 
This tool appears in a context where two types of tools exist but the differences between them is quite relevant.
One of the types of tools are the ones that do not support well the relational verification of programs that dynamically allocate mutable state; these take much weight off users' shoulders but are only useful to a restrict range of programs.
The other type of tools are the auto-active ones; these enable the verification of a considerably larger set of programs but require more user input.
WhyRel is implemented in OCaml and relies on a library provided by Why3 for constructing and pretty-printing WhyML parse trees.
This tool was evaluated by its authors through several case studies that demonstrate the effectiveness of relational region logic for alignment, expressing heap relations and relational reasoning that exploits encapsulation.

WhyRel is the most important practical work for this thesis.
Nevertheless, we will purposefully omit a considerable part of the details about WhyRel, since the verification of pointer programs is out of the scope of this thesis.
Therefore, the focus will be directed towards functional programs.


\subsection{Design details}
\label{subsec:whyrel_deisgn}

Some of the most relevant relational properties are conditional program equivalence, non-interference and sensitivity~\cite{barthe2019verifying}.
WhyRel addresses tooling that enables modular verification of relational properties of heap-manipulating programs, including the ones that act on differing data representations and involve dynamically allocated pointer structures.
It reaches modular reasoning about programs that contain pointers through local reasoning, using frame conditions and procedural and data abstraction.
Simpler relational invariants and specs can be achieved through the alignment of intermediate execution steps, a type of compositionality.

There are two approaches to relational properties verification supported by WhyRel.
One of them corresponds to the reduction of the verification to the proof of functional properties of the two source programs, but this method suffers from two problems.
It does not adapt well to complex programs and relational properties; it also prevents us to exploit similarities between the programs involved or reason through relational specs in a modular way.
The second technique uses appropriate program alignments to prove the desired relational property.
WhyRel uses biprograms to represent those alignments, that point out similarities between the two programs so that we can reason about their effects jointly.
Relational properties of a given alignment entail the corresponding relation between the source programs, in case the chosen alignment is adequate, i.e., it captures all pairs of executions of the two underlying programs.

WhyRel invests a lot in encapsulation, as a way to hide internal representation details from clients and to verify the modular correctness of a client independently of its internal organization.
Since this challenging on a technical level, WhyRel specifies encapsulation as a \emph{dynamic boundary}, a kind of dynamic frame.
A dynamic boundary captures the set of the module's internal locations.
Therefore, enforcing encapsulation in practice requires making sure that clients do not modify any internal locations of a module.
Relational logic has thorough soundness proofs~\cite{10.1145/3551497} and WhyRel, according to its authors, is an accurate implementation.

This tool should be interpreted as a front-end to Why3, where users provide the code, specifications and annotations and, additionally for relational verification, they should also provide relational specifications and alignments using a syntax for product programs.
After that, WhyRel translates these inputs into WhyML code, enconding them in a way that accurately captures the representation of the heap model and fine-grained framing formalized in relational region logic.
Finally, some VCs are added by WhyRel as intermediate assertions and lemmas for the user to confirm, and the verification itself is achieved mainly through the use of the Why3 IDE.


\subsection{Patterns of program alignment}
\label{subsec:whyrel_patterns}

Choosing alignments to decompose relational verification is an important task since it directly impacts how simple the relational assertions and loop invariants can be to prove the equivalence of the programs in question.
In this subsection, we present two examples of biprograms that capture alignments that are not maximal.
We will explain what that means below.


\subsubsection{Differing control structures}
\label{subsubsec:diff_control_structs}

\begin{figure}[h]
  \centering

  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang,
                        emph={meth, while, do, result, done, old, invariant, assert, int},
                        emphstyle=\ttfamily\bfseries\color{myorange}]
meth mult1(n: int, m: int) : int =
  i := 0;

  while (i < n) do
    j := 0;

    while (j < m) do
      result := result + 1;
      j := j + 1;
    done;

    i := i + 1;
  done;
    \end{lstlisting}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang,
                        emph={meth, while, do, result, done, old, invariant, assert, int},
                        emphstyle=\ttfamily\bfseries\color{myorange}]
meth mult2(n: int, m: int) : int =
  i := 0;

  while (i < n) do
    result := result + m;
    i := i + 1;
  done;
    \end{lstlisting}
  \end{subfigure}

  \caption{Two different programs that multiply two integer numbers.}
  \label{fig:mult_source_programs}
\end{figure}

In this~\cite{DBLP:conf/pldi/ChurchillP0A19} work, the authors developed a way to establish program equivalence through state-dependent alignments of program traces.
They identified a challenge when trying to confirm the equivalence of two programs that multiply two non-negative integer numbers using different control flow, as shown in \hyperref[fig:mult_source_programs]{the} figure.
The challenge arises in the context of automated approaches to relational verification specifically, because of the necessity of aligning an unbounded number \emph{m} of loop iterations on the left (\emph{mult1}) with a single iteration on the right (\emph{mult2}).

\begin{figure}[h]
  \centering
  \noindent
  \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, mathescape,
                      emph={meth, while, do, result, done, old, invariant, assert, int},
                      emphstyle=\ttfamily\bfseries\color{myorange}]
meth mult_bip (n: int, m: int | n: int, m: int) : (int | int) =
  $\lfloor$ i := 0 $\rfloor$;

  while (i < n) | (i < n) do
    invariant { i $\doteq$ i $\land$ result $\doteq$ result }

    (
      j := 0;
      while (j < m) do
        result := result + 1;
        j := j + 1;
      done
      |
      result := result + m;
    )

    assert { $\lhd$ result = old(result) + m $\lhd$ };
    $\lfloor$ i := i + 1 $\rfloor$;
  done;
  \end{lstlisting}
  \caption{An example of a biprogram for the example in \hyperref[fig:mult_source_programs]{this} figure.}
  \label{fig:mult_biprogram}
\end{figure}

In order to prove equivalence, we verify the biprogram presented in \hyperref[fig:mult_biprogram]{this} figure.
To achieve that, the pre-relation (or pre-condition) should be \(n \doteq n \land m \doteq m \) and the post-relation (or post-condition) \(result \doteq result\).
The $\doteq$ symbol means that the left side of the expression (only reasoning about the left program) is equal to the right side of the expression (this side also only reasons about the right program).
Therefore, the specification above means that if the input variables have the same values for both programs, then the output will also be the same.

The $|$ symbol means that the instructions on its left refer to the left program and the instructions at its right refer to the right program.
The $\lfloor$ and $\rfloor$ symbols mean that the instruction inside is executed by both sides.
This means that if we have $C_1 | C_2$ and $C_1$ is exactly the same instruction as $C_2$, we can simply write $\lfloor C_1 \rfloor$.

We call unary programs the source programs, in this case, \emph{mult1} and \emph{mult2} present in \hyperref[fig:mult_source_programs]{this} figure.
We say that an alignment is maximal if every statement/command is the same for the two unary programs.
This example's alignment is therefore not maximal, since we have two while loops on the left side and only one on the right side.
Nevertheless, we can still explore their similarities through the alignment of the outer loops in lockstep, and the left inner loop with the \emph{result := result+m;} instruction on the right.

Observe that the notation $\lhd F \lhd$ (respectively $\rhd F \rhd$) means that the unary formula \emph{F} is true for the state of the left (respectively right) program.
To prove that \emph{mult1} and \emph{mult2} are equivalent, we only need the invariant that shows equivalence on \emph{i} and \emph{result} for every iteration of the outer loop.
Additionally, to prove this invariant, we need to state that the inner loop of the left side has the same effect as the single assignement on the right: incrementing the \emph{result} by \emph{m}.
In this case, that reasoning comes to life through the assertion after the left inner loop and before the assigment to \emph{i}.
Notice that \emph{old(result)} inside the assertion represents the value that the $result$ variable had in the previous loop iteration.


\FloatBarrier
\subsubsection{Conditionally aligned loops}
\label{subsubsec:cond_align_loops_example}

\begin{figure}[h]
  \centering

  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang,
                        emph={meth, while, do, result, done, old, invariant, assert, int, if, then, end},
                        emphstyle=\ttfamily\bfseries\color{myorange}]
meth ex1 (x: int, n: int) : int =
  y := x;
  z := 24; // 4!
  w := 0;

  while (y > 4) do
    if (w mod n = 0) then
      z := z * y;
      y := y - 1;
    end;

    w := w + 1;
  done;

  result := z;
    \end{lstlisting}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang,
                        emph={meth, while, do, result, done, old, invariant, assert, int, if, then, end},
                        emphstyle=\ttfamily\bfseries\color{myorange}]
meth ex2 (x: int, n: int) : int =
  y := x;
  z := 16; // 2^4
  w := 0;

  while (y > 4) do
    if (w mod n = 0) then
      z := z * 2;
      y := y - 1;
    end;

    w := w + 1;
  done;

  result := z;
    \end{lstlisting}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.99\textwidth}
    \centering
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, mathescape,
                        emph={meth, while, do, result, done, old, invariant, assert, int, if, then, end},
                        emphstyle=\ttfamily\bfseries\color{myorange}]
meth ex_bip (x: int, n: int | x: int, n: int) : (int | int) =
  $\lfloor$ y := x $\rfloor$;
  (z := 24 | z := 16);
  $\lfloor$ w := 0 $\rfloor$;

  while (y > 4) | (y > 4) . $\lhd$ w mod n $\neq$ 0 $\lhd$ | $\rhd$ w mod n $\neq$ 0 $\rhd$ do
    invariant { $\lhd$ z $\lhd$ > $\rhd$ z $\rhd$ $\land$ y $\doteq$ y $\land$ $\rhd$ y $\geq$ 4 $\rhd$ }

    if (w mod n = 0) | (w mod n = 0) then
      (z := z * y | z := z * 2);
      $\lfloor$ y := y - 1 $\rfloor$;
    end;

    $\lfloor$ w := w + 1 $\rfloor$;
  done;

  $\lfloor$ result := z $\rfloor$;
    \end{lstlisting}
  \end{subfigure}

  \caption{Programs that compute the factorial, the exponent of x $\geq$ 4 and a biprogram.}
  \label{fig:cond_align_loops_ex}
\end{figure}

The example presented in the previous subsubsection takes advantage of the fact that the two outer loops are executed the same number of times, which can be inferred from their conditions being the same (\emph{i < n}).
It allows a lockstep alignment of the loops' iterations, leading to simple relational invariants.
However, we can not exploit this lockstep reasoning on all cases, forcing us to use other patterns of loop alignment.

Considering the example in the \hyperref[fig:cond_align_loops_ex]{figure}, observe that \emph{ex1} calculates the factorial of \emph{x} and \emph{ex2} computes the result of \emph{$2^x$}, both for \emph{x $\geq$ 4}.
In this case, we do not want to prove the equivalence of \emph{ex1} and \emph{ex2}, but that the factorial majorizes the exponent for \emph{x $\geq$ 4}.
Therefore, we provide the \hyperref[fig:cond_align_loops_rel_spec]{following} relational specification to reach that goal.
Keep in mind that the post-condition, indicated by the \emph{ensures} keyword, should be interpreted as "the value of the variable \emph{result} of \emph{ex1} is strictly greater than its \emph{ex2}'s counterpart, after the execution ends".

\begin{figure}[h]
  \centering
  \noindent
  \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, mathescape,
                      emph={meth, while, do, result, done, old, invariant, assert, int, requires, ensures, Both},
                      emphstyle=\ttfamily\bfseries\color{myorange}]
meth ex_bip_spec (x: int, n: int | x: int, n: int) : (int | int) =
  requires { x $\doteq$ x $\land$ $\rhd$ x $\geq$ 4 $\rhd$ $\land$ Both (n > 0) }
  ensures { $\lhd$ result $\lhd$ > $\rhd$ result $\rhd$ }
  \end{lstlisting}
  \caption{Relational spec for the example in \hyperref[fig:cond_align_loops_ex]{this} figure.}
  \label{fig:cond_align_loops_rel_spec}
\end{figure}

This relational spec means that, if at the start of the execution: both programs \emph{x's} are equal; \emph{ex2's x} is greater or equal to 4; and both \emph{n's} are strictly greater than 0, then the output of the left program will be strictly greater than the output of the right side.

\emph{ex1} and \emph{ex2} structure's are very similar and the example is simplified by setting the \emph{z} to 4! and $2^4$, respectively.
The \emph{w} variables' goal is to introduce \emph{stuttering} steps.
In this case, it restricts the loops to only execute the assignments to \emph{z} and \emph{y} when \emph{w} is divisible by \emph{n}, something that is valid for both sides.

Since the only restriction on \emph{n} is that they both sould be greater than 0, we can not be sure if the number of iterations will be the same for both sides, which excludes the possibility of providing an alignment based on pure lockstep.
Therefore, we should only align iterations in lockstep when \emph{w mod n = 0} on both sides; in other words, when the assignments to \emph{z} and \emph{y} will be performed.
For the other situations, when a side will not update those variables in the current iteration, the alignment of the iteration on the other side should be done with executing nothing, represented by the \emph{skip} command.
With these two types of alignments, we are able to establish the invariant $\lhd$ z $\lhd$ > $\rhd$ z $\rhd$.

This alignment is represented in terms of syntax by the \emph{ex\_bip} biprogram in \hyperref[fig:cond_align_loops_ex]{this} figure.
Conditional alignment is captured using additional annotations, the \emph{alignment guards}.
These annotations are general relation formulas that express conditions that cause the iterations to be either left-only, right-only or lockstep.
In our example, the alignment guards are \emph{$\lhd$ w mod n $\neq$ 0 $\lhd$ | $\rhd$ w mod n $\neq$ 0 $\rhd$}.
Looking at the left alignment guard, \emph{$\lhd$ w mod n $\neq$ 0 $\lhd$}, we understand that \emph{ex\_bip} executes left-only iterations when, on the left side, \emph{w} is \textbf{not} divisible by \emph{n}. 
A left-only iteration means that only the left side statements inside the loop are executed: \emph{if (w mod n = 0) then (z := z*y; y := y-1) end; w := w+1}.
The only assignment in this case is the increment of the \emph{w} variable, since the alignment guard is contraditory with the $if$'s condition.
This reasoning is similar to the right side, representing the right-only iterations.
Finally, the lockstep iterations happen when both alignment guards are false; in this case, when \emph{w} is a multiple of \emph{n} on both sides.
This alignment allows us to use the relational invariants in \emph{ex\_bip} to prove that the post-condition holds after the execution of the biprogram.


\FloatBarrier
\subsection{Translating biprograms into product programs}
\label{subsec:whyrel_translation}

One of the main goals of WhyRel (also one of our main goals) is translating biprograms into product programs.
This tool achieves that through WhyML functions that act on a pair of states; WhyML functions that encode biprograms also act on a refperm, renaming references allocated in the two states being related.
However, before the translation phase, WhyRel checks how adequate the biprogram in question is.
At an intuition level, we can think that if the two unary programs are related in accord with a relational spec, then the biprogram is adequate if it satisfies that spec, which suggests that relatedness strongly impacts the correctness of the biprogram.

In practice, WhyRel confirms the adequacy of the biprogram in two phases, by checking if it can cover all pairs of executions of its two source programs.
The first phase is done by checking the syntax in order to ensure that the biprogram was really constructed from its underlying programs, through projection operations.
Consider the following notation, where \emph{CC} represents a biprogram, $\wideleftharpoonup{CC}$ its left projection (the left unary program) and $\widerightharpoonup{CC}$ its right projection (the right unary program).
As a simple example, the left projection of the following program is \textbf{x := 2; x := x + 1;} and the right projection is \textbf{x := x + 1;}.

\begin{lstlisting}[style=while_lang, mathescape,
                      emph={skip},
                      emphstyle=\ttfamily\bfseries\color{myorange}]
( x := 2 | skip);
$\lfloor$ x := x + 1 $\rfloor$;
\end{lstlisting}

Considering the unary original programs \emph{C} and \emph{C'}, the corresponding biprogram \emph{CC} and $\equiv$ the symbol for syntactic equality, WhyRel checks $\wideleftharpoonup{CC}$ $\equiv$ \emph{C} and $\widerightharpoonup{CC}$ $\equiv$ \emph{C'}.
The second phase consists in the introduction of annotations in the biprogram, such as assertions or loop invariants, by WhyRel, with the objective of ensuring adequacy.
In an hypothetical case where a biprogram aligns two loops in lockstep, this tool would add, as relational loop invariant, the equivalence of the two loop guards.

\begin{figure}[ht]
  \centering
  \begin{minipage}{\linewidth}
    \setlength{\tabcolsep}{8pt}
    \begin{tabularx}{\linewidth}{@{}L R@{}}
      \textbf{1. } $\beta\llbracket C \mid C' \rrbracket(\tau_l,\tau_r)$ &
      \parbox[t]{\hsize}{$\triangleq\ \mu\llbracket C\rrbracket(\tau_l);\ \mu\llbracket C'\rrbracket(\tau_r)$} \\[6pt]
      \\

      \textbf{2. } $\beta\llbracket \lfloor m(x\mid y) \rfloor \rrbracket(\tau_l,\tau_r)$ &
      \parbox[t]{\hsize}{$\triangleq\ \phi(m)\big(\tau_l.\mathrm{st},\tau_r.\mathrm{st},
      \epsilon\llbracket x\rrbracket(\tau_l),\epsilon\llbracket y\rrbracket(\tau_r)\big)$} \\[6pt]
      \\

      \textbf{3. } $\beta\llbracket \lfloor C \rfloor \rrbracket(\tau_l,\tau_r)$ &
      \parbox[t]{\hsize}{$\triangleq\ \beta\llbracket C \mid C \rrbracket(\tau_l,\tau_r)$} \\[6pt]
      \\

      \textbf{4. } $\beta\llbracket CC;\,DD\rrbracket(\tau_l,\tau_r)$ &
      \parbox[t]{\hsize}{$\triangleq\ \beta\llbracket CC \rrbracket(\tau_l,\tau_r);\ \beta\llbracket DD \rrbracket(\tau_l,\tau_r)$} \\[6pt]
      \\

      \textbf{5. } $\beta\llbracket \kw{var }x:T \mid x:T' \ \kw{ in } CC\rrbracket(\tau_l,\tau_r)$ &
      \parbox[t]{\hsize}{%
        $\triangleq\ \kw{let}\ x_l=\mathrm{def}(T)\ \kw{in}\ \kw{let}\ x_r=\mathrm{def}(T')\ \kw{in}$\\[2pt]
        \hspace{2.2em}\text{  }\text{  }\text{  }\text{  } $\beta\llbracket CC \rrbracket\big([\tau_l\mid x:x_l],[\tau_r\mid x:x_r]\big)$
      } \\[8pt]
      \\

      \textbf{6. } $\beta\llbracket \kw{if } E\mid E' \ \kw{then}\ CC\ $ $\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{ } \kw{else}\ DD\rrbracket(\tau_l,\tau_r)$ &
      \parbox[t]{\hsize}{%
        $\triangleq\ \kw{assert}\{\ \epsilon\llbracket E\rrbracket(\tau_l)=\epsilon\llbracket E'\rrbracket(\tau_r)\ \};$\\[2pt]
        \hspace{2.2em}\text{  }\text{  }\text{  }\text{  } $\kw{if}\ \epsilon\llbracket E\rrbracket(\tau_l)\ \ \kw{ then}\ \beta\llbracket CC\rrbracket(\tau_l,\tau_r)$\\[2pt]
        \hspace{2.2em}\text{  }\text{  }\text{  }\text{  } $\kw{ else}\ \beta\llbracket DD\rrbracket(\tau_l,\tau_r)$
      } \\[8pt]
      \\

      \textbf{7. } $\beta\llbracket \kw{while } E\mid E' \ \kw{do}\ CC\rrbracket(\tau_l,\tau_r)$ &
      \parbox[t]{\hsize}{%
        $\triangleq\ \kw{while}\ \epsilon\llbracket E\rrbracket(\tau_l)\ \kw{do}$\\[2pt]
        \hspace{2.2em}\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  } $\kw{invariant}\ \{\ \epsilon\llbracket E\rrbracket(\tau_l)=\epsilon\llbracket E'\rrbracket(\tau_r)\ \}$\\[2pt]
        \hspace{2.2em}\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  } $\beta\llbracket CC\rrbracket(\tau_l,\tau_r)$
      } \\[8pt]
      \\

      \textbf{8. } $\beta\llbracket \kw{while } E\mid E' .\ P\mid P' \ \kw{do}\ CC\rrbracket(\tau_l,\tau_r)$ &
      \parbox[t]{\hsize}{$\triangleq$} \\[2pt]
    \end{tabularx} 

    \vspace{2pt} 
    \noindent\begin{minipage}[t]{\linewidth} 
      \raggedright
      \hspace{1.1em}$\kw{while}\ (\epsilon\llbracket E\rrbracket(\tau_l)\ \lor\ \epsilon\llbracket E'\rrbracket(\tau_r))\ \kw{do}\ \kw{invariant}\ \{A\}$\\[2pt]
      \hspace{2.2em}$\kw{if}\ (\epsilon\llbracket E\rrbracket(\tau_l)\ \land\ $F$\llbracket P\rrbracket(\tau_l,\tau_r))\ \kw{then}\ \mu\llbracket \wideleftharpoonup{CC}\rrbracket(\tau_l)$\\[2pt]
      \hspace{2.2em}$\kw{else if}\ (\epsilon\llbracket E'\rrbracket(\tau_r)\ \land\ $F$\llbracket P'\rrbracket(\tau_l,\tau_r))\ \kw{then}\ \mu\llbracket \widerightharpoonup{CC}\rrbracket(\tau_r)$\\[2pt]
      \hspace{2.2em}$\kw{else}\ \beta\llbracket CC\rrbracket(\tau_l,\tau_r)$\\[2pt]
      \hspace{1.2em}\text{where } $A \equiv (\epsilon\llbracket E\rrbracket(\tau_l)\ \land\ $F$\llbracket P\rrbracket(\tau_l,\tau_r)) \lor (\epsilon\llbracket E'\rrbracket(\tau_r)\ \land\ $F$\llbracket P'\rrbracket(\tau_l,\tau_r))\ \lor$\\[2pt]
      \hspace{4.0em}\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  }\text{  } $(\neg\epsilon\llbracket E\rrbracket(\tau_l)\ \land\ \neg\epsilon\llbracket E'\rrbracket(\tau_r)) \lor (\epsilon\llbracket E\rrbracket(\tau_l)\ \land\ \epsilon\llbracket E'\rrbracket(\tau_r))$
    \end{minipage}
    \vspace{2pt}

  \end{minipage}
  \caption{Rules of translation of biprograms.}
  \label{fig:translation-biprograms-rules}
\end{figure}

The rules of translation of biprograms into product programs are listed in \hyperref[fig:translation-biprograms-rules]{this} figure.
$\beta$ receives a biprogram and a pair of contexts $(\tau_l,\tau_r)$ and transforms it into a WhyML program.
Contexts map WhyRel identifiers to WhyML identifiers and store information about the state parameters that the generated WhyML will receive.
In a similar way, $\mu$ translates unary programs into WhyML programs, $\epsilon$ map expressions into WhyML expressions and $F$ transforms a restricted set of relation formulas also into WhyML expressions.
Unary programs are not restricted to operate on a disjoint set of variables but, if they do not, during translation, WhyRel naturally has to rename those variables' names. 
Rule 5 performs this renaming, where context $\tau_l$ (and respectively $\tau_r$) is extended with the renaming of $x$ with its copy $x_l$ (and respectively $\tau_r$ is extended with the binding \emph{x : x\_r}).

Rule 1 sequentially composes the unary translations of $C$ and $C'$.
$\lfloor C \rfloor$ is simply syntactic sugar for ($C|C$) that are translated by rule 3, with the expection of method calls.
Aligning method calls that establish that the relational spec associated with the method allow procedure-modular reasoning about relational properties.
Using the $\phi$ symbol to represent a global method context, WhyRel translates these calls to calls to the correct WhyML product program (rule 2).

Rule 4 represents that a sequence of commands in the biprogram is translated to also a sequence of commands in the product program.
Control flow statements require the generation of more proof obligations: in the case of the \emph{if...then...else}, in rule 6, it appears as a runtime assertion; in a lockstep aligned loop (rule 7), it takes the form of a loop invariant stating that the guards are in agreement.
Regarding conditionally aligned loops, described by rule 8, the pattern shown by the alignment guards $P | P'$ are captured by the generated loop body.
This means that if the left (resp. right) guard evalutes to true and $P$ (resp. $P'$) holds, a left-only (resp. right-only) iteration is executed; if not, a lockstep iteration is performed.

The adequacy of the biprogram in this case is guaranteed by the requirement that $A$ must be an invariant.
$A$ states that the loop can either perform a one-sided iteration or a lockstep until both sides finish.
The alignment guards $P$ and $P'$ can be any relational formula, in the context of relational region logic.
Nonetheless, the encoding of conditionally aligned loops regards a conditional that branches on these alignment guards.
In Why3, $P$ and $P'$ have to be restricted for this to function correctly; they can not contain quantifiers, for example.
WhyRel supports several types of elements to be in the alignment guards: the commonly used boolean connectives, one-sided boolean expressions, one-sided points-to-assertions and, as referred several times before, agreement formulas.


\subsubsection{Translation example}
\label{subsubsec:whyrel_translation_example}

Recall the multiplication \hyperref[fig:mult_source_programs]{unary programs} and their \hyperref[fig:mult_biprogram]{biprogram}.
We now present its corresponding product program, after applying the translation rules described before.
In order to demonstrate what would be the expected output of the tool when we feed it the \hyperref[fig:mult_biprogram]{biprogram}, we developed a simplified and more readable version of \href{https://github.com/dnaumann/RelRL/blob/main/examples/equiv-check/example.mlw}{this} example.

\begin{figure}[h]
  \centering
  \noindent
  \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, mathescape,
                      emph={module,use,let,requires,ensures,in,while,do,variant,invariant,done,return,label,end,assert,at},
                      emphstyle=\ttfamily\bfseries\color{myorange}]
module MultPP
  use int.Int
  use ref.Ref

  let mult_whyml (n_l m_l n_r m_r : int) : (result_l: int,
    result_r: int)
    requires { n_l = n_r && n_l >= 0 && m_l = m_r && m_l >= 0 }
    ensures  { result_l = result_r }
  =
    let i_l = ref 0 in
    let i_r = ref 0 in
    let result_l = ref 0 in
    let result_r = ref 0 in

    while !i_l < n_l do
      variant { n_l - !i_l }
      invariant { !i_l = !i_r && !result_l = !result_r }

      label Old in
      let j = ref 0 in
      
      while !j < m_l do
        variant { m_l - !j }
        invariant { 0 <= !j <= m_l && !result_l = !result_r + !j }

        result_l := !result_l + 1;
        j := !j + 1
      done;

      result_r := !result_r + m_r;

      assert { !result_l = (!result_l at Old) + m_l };
      i_l := !i_l + 1;
      i_r := !i_r + 1;
    done;
    
    return !result_l, !result_r
end
  \end{lstlisting}
  \caption{$mult$ product program (WhyML).}
  \label{fig:translation_ex}
\end{figure}

Firstly, we had to add the pre-conditions \emph{n\_l >= 0} and \emph{m\_l >= 0}, the two variants and the invariants for the inner loop, since Why3 was not able to prove correctness without these extra pieces of specification.
\emph{n\_l >= 0} is needed because if \emph{n\_l < 0}, the program would wrongly return \emph{0, 0} every time.
The second additional pre-condition is necessary to stop $result\_r$ from eventually reaching its correct value (assuming $n\_r$ is positive) while $result\_l$'s value stays at 0 during the whole execution of the program.
The \emph{variant \{ n\_l - !i\_l \} and variant \{ m\_l - !j \}} are needed to prove termination of the outer and inner loops, respectively.
The \emph{invariant \{ 0 <= !j <= m\_l \&\& ! result\_l = ! result\_r + !j \}} is also not dispensable, since Why3 is not able to reason automatically about how $result\_l$ varies during the inner loop.

Additionally, because \emph{old(result)} in WhyRel's biprogram language means the value that the $result$ variable had in the previous loop iteration, but in WhyML it represents the value in the previous function call, we had to adapt this when translating.
The best solution we found is the use of labels to capture the state of program before the execution of the inner while loop, therefore allowing us to access the value of $result$ in the previous iteration.
Finally, with this specification, Z3 was able to discharge all VCs automatically and the proof took 0.04 seconds to complete.

\begin{table}[!h]
\begin{center}
  \begin{tabular}{|l|l|l|l|c|}
    \hline \multicolumn{2}{|c|}{Proof obligations } & \provername{Z3 4.14.1} \\ 
    \hline
    \explanation{VC for mult\_pp}  & \explanation{loop invariant init} & \valid{0.00} \\ 
    \cline{2-3}
    & \explanation{loop invariant init} & \valid{0.01} \\ 
    \cline{2-3}
    & \explanation{loop variant decrease} & \valid{0.00} \\ 
    \cline{2-3}
    & \explanation{loop invariant preservation} & \valid{0.01} \\ 
    \cline{2-3}
    & \explanation{assertion} & \valid{0.01} \\ 
    \cline{2-3}
    & \explanation{loop variant decrease} & \valid{0.00} \\ 
    \cline{2-3}
    & \explanation{loop invariant preservation} & \valid{0.01} \\ 
    \cline{2-3}
    & \explanation{postcondition} & \valid{0.00} \\ 
    \hline 
  \end{tabular}
  \caption{\hyperref[fig:translation_ex]{$mult\_pp$} verification results.}
\end{center}
\end{table} 
