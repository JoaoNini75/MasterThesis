%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter3.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter3.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother


\chapter{State of the Art}
\label{cha:state_of_the_art}

\textbf{Definition: Structural equivalence.} We say that two programs are structurally equivalent if they present the same sequence statements/commands.
For example, a program \emph{$P_1$} that consists in two assignments and a while loop is not equivalent to another program program \emph{$P_2$} if the latter's structure is a single for loop.
On the other hand, if \emph{$P_1$} and \emph{$P_2$} both contain, let us say, three assignments followed by a for loop (with a single assignment inside) that executes the same number of iterations, they are structurally equivalent, despite the values being assigned to the variables.

Note that this definition is useful when reading about self-composition and cross-products below in this capter.
However, we aim to allow verification of programs that would not be considered by this definition of structural equivalence and therefore be more flexible, but we still need some degree of similarity.
That is why we do not target programs that include exceptions.


\section{Self-composition} 
\label{sec:self_composition}

\subsection{Introduction}
\label{subsec:self_composition_intro}

The proposal of self-composition ~\cite{DBLP:conf/csfw/BartheDR04} is to offer an extensible and flexible way of controlling information flow.
This is usually done by information flow type systems, but they suffer from the issues that self-composition attempted to solve.
In the work mentioned above, the authors tried to address the static enforcement of information flow policies but focused on a specific one called non-interference.
Non-interference separates the state of a program into a public and a secret part and, by observing its execution, determines if there was a information leak on the secret part of the state.

Despite the well-defined focus, the efforts of the work specified before apply to several programming languages and different definitions of security, even including declassification, partially.
In the end, the authors were able to establish a general theory of self-composition to prove that programs are non-interfering.
One of the main features of self-composition is its expressiveness and that there is no need to prove the soundness of type systems, since it is based on logic.


\subsection{Exploring non-interference}
\label{subsec:exploring_non_interference}

In order to give an example of self-composition in the next subsection, we present now a simple program that respects the properties of termination-insensitivity and non-interference.

Consider a simple, deterministic and imperative language that allows sequential composition and the evaluation relation \(\langle P, u\rangle \Downarrow v\), where \(P\) is a program and \(u,v\) are memories.
Memory in this context means a map where the keys are the program variables of \(P\) and the values are the values of those program variables.
Furthermore, consider that all variables of \(P\) need to be either public or private, let \(\overrightarrow{x}\) be the set of all public variables in \(P\) and \(\overrightarrow{y}\) the set of all of its private variables.
For all memories \(u_1,u_2,v_1,v_2\), the properties of termination-insensitivity and non-interference for \(P\) can be described as:

\[ [\langle P, u_1\rangle \Downarrow v_1 \, \land \, \langle P, u_2\rangle \Downarrow v_2 \, \land \, u_1 {=}_{L} u_2] \Rightarrow v_1 {=}_{L} v_2 \]

where ${=}_{L}$ is the point-wise extension of equality on values to public parts of memories.

Consider [\(\overrightarrow{x'}, \overrightarrow{y'} / \overrightarrow{x}, \overrightarrow{y}\)] as being a renaming of the program variables \(\overrightarrow{x}\) and \(\overrightarrow{y}\) of \(P\) with fresh variables \(\overrightarrow{x'}\), \(\overrightarrow{y'}\) and let \(P'\) be as \(P\) but with its variables renamed: \(P\)[\(\overrightarrow{x'}, \overrightarrow{y'} / \overrightarrow{x}, \overrightarrow{y}\)].
Also, to refer to the the disjoint union of two memories we will use the $\uplus$ symbol.
Therefore, we have \(\langle P, u\rangle \Downarrow v \, \land \, \langle P', u'\rangle \Downarrow v' \) iff \(\langle P; P, u \uplus u' \rangle \Downarrow v \uplus v'\).
We can now rearrange non-interference, for memories \(u,u',v,v'\), as the following:

\[ (\langle P; P, u \uplus u' \rangle \Downarrow v \uplus v' \, \land \, u = {}_{\overrightarrow{x}} u' \circ [\overrightarrow{x} / \overrightarrow{x'}]) \Rightarrow v = {}_{\overrightarrow{x}} v' \circ [\overrightarrow{x} / \overrightarrow{x'}] \]

where $\circ$ is the symbol for function composition and \(= {}_{\overrightarrow{x}}\) is the point-wise extension of equality on values to the restriction of memories to $\overrightarrow{x}$.
With this formulation, we are able to reduce the non-interference property of \(P\) to a property of all the executions of \(P; P'\).
Therefore, an alternative characterization of non-interference can be presented by using programming logics, since they are sound and relatively complete with relation to the operational semantic.
We can describe non-interference using Hoare triples:

\[ \{\overrightarrow{x} = \overrightarrow{x'}\} \, P; P' \, \{\overrightarrow{x} = \overrightarrow{x'}\} \]


\subsection{Code Example}
\label{subsec:self_composition_example}

Consider the following program:

\[ x := y; \, x := 0 \]

and \(x \mapsto x'\) and \(y \mapsto y'\) as the renaming functions. 
The program is non-interferent iff

\[ \{x = x'\} \, x := y; x := 0 \, x' := y'; x' := 0 \, \{x = x'\} \]

We are able to characterize information flow policies, which include some types of declassification, through the replacement of the =- relation by other (partial) equivalence relation.
More generally, this form of characterization allows us to use existing verification tools to prove (or disprove) information flow policies for a given program.


\FloatBarrier
\section{Cross-products} 
\label{sec:cross_products}

Cross-products~\cite{DBLP:conf/fm/ZaksP08} aim to prove program equivalence, with a strong focus on verifying compiler optimizations.
Instead of proving that both programs are equivalent, the analysis is done by combining the two input programs into one: the cross-product.
With them, instead of recurring to program analysis and proof rules developed specifically for translation validation, it became possible to use existing methods and tools to prove properties of a single program. 
Despite handling the most common intraprocedural compiler optimizations, cross-products can not be applied to two input programs with dissimilar structures, which is a major constraint of this work.

One important aspect of CoVaC, a framework developed by the authors of the aforementioned article, is that it was made to validate program equivalence in general while balancing precision and efficiency.
The approach was to divide the analysis in two phases, a faster one first and a more precise after that.
The first phase utilizes a fast, yet imprecise value numbering algorithm whose results are used to determine if it is necessary to apply the second phase. 
This final part is based on assertion checking, a static program verification method which, under the hood, computes the weakest-precondition~\cite{DBLP:books/ph/Dijkstra76} using CVC3~\cite{cvc3}. 

The results presented on the cited work reveal that CoVaC was able to verify a very considerable set of optimizations of LLVM~\cite{llvm}, a complex modern compiler.
Yet, the tool does not support verification of interprocedural optimizations or information flow policies, for example, but these are limitations that the authors showed interest in addressing.


\FloatBarrier
\section{Product Programs} 
\label{sec:product_programs}

\subsection{Motivation} 
\label{subsec:product_programs_motivation}

Relational Hoare Logic serves as a good starting point to compare the behavior of two different executions of the same program or even two different programs.
However, there are few available tools and practical reasoning logics for relational verification.
One of the main limitations of the existing ones~\cite{DBLP:conf/popl/Benton04, DBLP:journals/tcs/Yang07} is the constraint of \emph{structural equivalence}.
Inversely, traditional program verification has diverse and extensive tool support.
Therefore, as a way of getting around that limitation, we can take advatange of the existing tool support if we are able to soundly reduce relational verification tasks into standard verification ones.
This means we would translate Hoare quadruples (\{$\Phi$\} \emph{$P_1$} \emph{\textasciitilde} \emph{$P_2$} \{$\Psi$\}) into Hoare triples (\{$\phi$\} \emph{P} \{$\psi$\}), making sure that: if the original quadruple is valid, then the generated triple is also valid; if the if the original quadruple is not valid, the generated triple would also be invalid.
Considering $\vDash$ the symbol for validity, the objective is finding \emph{$\phi$, P, $\psi$} that:
\[ \vDash \{\Phi\} \, \textbf{$P_1$} \sim \textbf{$P_2$} \, \{\Psi\} \ \ \ \rightarrow \ \ \ \ \ \vDash \{\phi\} \, \textbf{P} \, \{\psi\} \]

Let us consider two imperative and \emph{separable} programs \emph{$P_1$} and \emph{$P_2$}.
This enables the capacity of the assertions to be seen as first-order formulas about the variables of \emph{$P_1$} and \emph{$P_2$}.
Self-composition~\cite{DBLP:conf/csfw/BartheDR04} represents a way of establishing the wanted equalities mentioned above: \emph{P $\equiv$ $P_1$;$P_2$}, \emph{$\phi$ $\equiv$ $\Phi$} and \emph{$\psi$ $\equiv$ $\Psi$}.
Despite being characterized by soundness and relative completeness~\cite{DBLP:journals/jlp/BartheCK16}, this construction is impractical for the authors of this~\cite{DBLP:conf/sas/TerauchiA05} work.
One of the two main reasons is that the existing automatic safety analysis tools available are not powerful enough to verify most of realistic problems and, when they can, there is a lack of performance.
The other reason is that the safety analysis developed for naturally 1-safety problems is not expected to advance significantly in the foreseeable future.

There is another relevant method previously discussed in this \hyperref[sec:cross_products]{this} section, the cross-products.
These suffer from the constraint of structural equivalence, making impossible to reason about properties or program optimizations that are based on different control flows.

Product Programs~\cite{DBLP:conf/fm/BartheCK11} appear to be the best path to follow, since they represent a general notion that combines the flexibility of self-composition in executing asynchronous steps and the efficiency of cross-products when it comes to treat synchronous steps. 
Product programs are a way to reduce relational verification into standard verification through the construction of a product program \emph{P} that simulates the execution statements of any two programs \emph{$P_1$} and \emph{$P_2$}.


\FloatBarrier
\subsection{From relational verification to standard verification} 
\label{subsec:product_programs_relverif_to_stdverif}

\subsubsection{A first intuition} 
\label{subsubsec:product_programs_intuition}

As a way to provide a first intuition of construction of a product from structurally dissimilar programs before we dive into the definitions and rules later in this subsection, consider the simple \hyperref[fig:pp_first_intuition]{example} below (assume 0 $\leq$ N).

\begin{figure}[h]
  \centering

  \begin{subfigure}[t]{0.28\textwidth}
    \centering
    \makebox[\linewidth][l]{\textbf{Source program}}\par\smallskip
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, linewidth=\linewidth, xleftmargin=0pt, framexleftmargin=0pt]
@i := 1@; 
while (@i (*$\textcolor{red}{\leq}$*) N@) do
  @x += i@;
  @i++@
    \end{lstlisting}
  \end{subfigure}\hfill%
  \begin{subfigure}[t]{0.28\textwidth}
    \centering
    \makebox[\linewidth][l]{\textbf{Transformed program}}\par\smallskip
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, linewidth=\linewidth, xleftmargin=0pt, framexleftmargin=0pt]
.j := 1.; 
while (.j (*$\textcolor{blue}{\leq}$*) N.) do
  .y += j.;
  .j++.
    \end{lstlisting}
  \end{subfigure}\hfill%
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \makebox[\linewidth][l]{\textbf{Product program}}\par\smallskip
    \noindent
    \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang, linewidth=\linewidth, xleftmargin=0pt, framexleftmargin=0pt]
@i := 1@; @x += i@; @i++@; .j := 1.;
while (@i (*$\textcolor{red}{\leq}$*) N@) do
  @x += i@; @i++@;
  .y += j.; .j++.
    \end{lstlisting}
  \end{subfigure}

  \caption{First intuition source, transformed and product programs.}
  \label{fig:pp_first_intuition}
\end{figure}

The first step to build the product program is to unroll the first loop iteration of the source code before synchronizing the loop statements.
Rather than relying plainly on self-composition, this idea maximizes synchronization and differs from a functional interpretation of the product components.
Furthermore, this approach also reduces considerably the complexity of the invariants required to prove the product program.
In the case of a sequential composition of the source and transformed programs, we would need to provide invariants of the form \emph{x = X + $\frac{i(i-1)}{2}$} and \emph{y = Y + $\frac{j(j-1)}{2}$}, respectively, assuming the pre-conditions \emph{x = X} and \emph{y = Y}.
On the other hand, if we construct the product program, the loop invariant \emph{i = j $\land$ x = y} is enough to verify that the two first programs above satisfy the pre and post-relation \emph{x = y}.


\FloatBarrier
\subsubsection{Establishing the ground rules} 
\label{subsubsec:product_programs_ground_rules}

The commands in our programming model will stick to the grammar defined previously \hyperref[sub:relational_hoare_logic_formal_proof_rules]{here}, but we redefine some of its components in a more specific way.

\emph{x} ranges over a set of integer variables V$_i$, \emph{a} ranges over a set of array variables V$_a$, let V denote V$_i$ $\cup$ V$_a$, and assuming V$_i$ $\cap$ V$_a$ = $\emptyset$.
Let \emph{e} $\in$ AExp and \emph{b} $\in$ BExp range over integer and boolean expressions.
Their semantics are given by ${(\llbracket e \rrbracket)}_{e \in AExp}$ : S $\rightarrow$ $\mathbb{Z}$ and ${(\llbracket b \rrbracket)}_{b \in BExp}$ : S $\rightarrow$ $\mathbb{B}$, respectively.
Execution states' representation is \emph{S = (V$_i$ $\rightharpoonup$ $\mathbb{Z}$) X (V$_a$ $\rightharpoonup$ ($\mathbb{Z}$ $\rightharpoonup$ $\mathbb{Z}$)).}
The semantics of the commands are standard, deterministic and are based on the relation $\langle$c, $\sigma$$\rangle$ $\rightsquigarrow$ $\langle$c', $\sigma$'$\rangle$.

Notice that $\langle$\textbf{skip}, $\sigma$$\rangle$ marks the end of the programs, \textbf{assert}(b) blocks the execution of the program if \emph{b} is false, and we let $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' mean $\langle$c, $\sigma$$\rangle$ $\rightsquigarrow$* $\langle$\textbf{skip}, $\sigma$'$\rangle$.
An assertion $\phi$ is a first-order formula with variables in V.
We let $\llbracket\phi\rrbracket$ denote the set of states satisfying $\phi$.
Finally, we let var(\emph{c}) $\subseteq$ V and var($\phi$) $\subseteq$ V denote the set of (free) variables of a command c and assertion $\phi$, respectively.

As we discussed before in this document, two commands are \emph{separable} if they operate on disjoint sets of variables.
Similarly, we consider that two states are \emph{separable} if their domains are disjoint.
Consider $\mu$$_1$ $\uplus$ $\mu$$_2$ represent the union of finite maps:
\[
(\mu_1 \uplus \mu_2) \ x =
\begin{cases}
    \mu_1 x & \text{if } x \in \operatorname{dom}(\mu_1) \\
    \mu_2 x & \text{if } x \in \operatorname{dom}(\mu_2)
\end{cases}
\]

and overload this notation for the union of separable states ($\mu$,$\nu$) $\uplus$ ($\mu$',$\nu$'), whose definition is ($\mu$ $\uplus$ $\mu$', $\nu$ $\uplus$ $\nu$').
Taking into account that we assume that the states are separable, another way of looking at assertions is by viewing them as relations on states: ($\sigma$$_1$, $\sigma$$_2$) $\in$ $\llbracket$$\phi$$\rrbracket$ iff $\sigma$$_1$ $\uplus$ $\sigma$$_2$ $\in$ $\llbracket$$\phi$$\rrbracket$.
Therefore, the definition below illustrates the formal statement of valid relational specifications.
\bigskip

\textbf{Definition 1 -}  \emph{Two commands $c_1$ and $c_2$ satisfy the pre-condition $\phi$ and the post-condition $\psi$ described by a valid Hoare quadruple if,
                                for all states $\sigma$$_1$, $\sigma$$_2$, $\sigma$$_1$', $\sigma$$_2$' such that $\sigma_1$ $\uplus$ $\sigma_2$ $\in$ $\llbracket$$\phi$$\rrbracket$
                                and $\langle$$c_1$, $\sigma$$_1$$\rangle$ $\Downarrow$ $\sigma$$_1$' and $\langle$$c_2$, $\sigma$$_2$$\rangle$ $\Downarrow$ $\sigma$$_2$', we have $\sigma_1$' $\uplus$ $\sigma_2$' $\in$ $\llbracket$$\psi$$\rrbracket$.}

\bigskip
Since our objective is to reduce the validity of Hoare quadruples to validity of Hoare triples, we must say that we establish our notion of valid Hoare triple as stronger than usual.
It requires that the command allows the program to finish its execution, i.e., the command is \emph{non-stuck}.
\bigskip

\textbf{Definition 2 -}  \emph{A Hoare triple {$\phi$} c {$\psi$} is valid ($\vDash$ \{$\phi$\} c \{$\psi$\}) if c is $\phi$-nonstuck and for all $\sigma$, $\sigma$' $\in$ S, $\sigma$ $\in$ $\llbracket$$\phi$$\rrbracket$ and $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' imply $\sigma$' $\in$ $\llbracket$$\psi$$\rrbracket$.}

\bigskip
This notion of validity requires, however, that we extend Hoare logic for it to be able to treat \textbf{assert} statements.
The necessary rule is:
\begin{figure}[h]
  \begin{center}
    \begin{minipage}{\linewidth}
      \centering
      \begin{mathpar}
        \inferrule*[] { } {\vdash \ \{ \textit{b} \land \Phi \} \ \textbf{assert}\text{(b)} \ \{ \Phi \}}
      \end{mathpar}
    \end{minipage}
  \end{center}
\end{figure}


\FloatBarrier
\subsubsection{Construction of Product Programs} 
\label{subsubsec:product_programs_construction}

Firstly in this section, we define the rules that will be used to deal with structurally equivalent programs. 
After that, we increment that set with structural transformations to allow the treatment of programs with different structures.

\begin{figure}[h]
  \centering
  \begin{mathpar}

  \inferrule*[]
    { }
    {c_1 \times c_2 \; \rightarrow \; c_1 ; c_2}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c \\ 
     c'_1 \times c'_2 \; \rightarrow \; c'}
    {(c_1 ; c'_1) \times (c_2 ; c'_2) \; \rightarrow \; c ; c'}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c}
    {(\textbf{while} \; b_1 \; \textbf{do} \; c_1) \times (\textbf{while} \; b_2 \; \textbf{do} \; c_2) \; \rightarrow \;
    \textbf{assert}(b_1 \Leftrightarrow b_2); \; \textbf{while} \; b_1 \; \textbf{do} \; (c ; \textbf{assert}(b_1 \Leftrightarrow b_2))}

  \inferrule*[]
    {c_1 \times c_2 \; \rightarrow \; c \\ 
    c'_1 \times c'_2 \; \rightarrow \; c'}
    {(\textbf{if} \; b_1 \; \textbf{then} \; c_1 \; \textbf{else} \; c'_1) \times (\textbf{if} \; b_2 \; \textbf{then} \; c_2 \; \textbf{else} \; c'_2) \; \rightarrow \;
    \textbf{assert}(b_1 \Leftrightarrow b_2); \; \textbf{if} \; b_1 \; \textbf{then} \; c \; \textbf{else} \; c'}

  \inferrule*[]
    {c_1 \times c \; \rightarrow \; c'_1 \\ 
    c_2 \times c \; \rightarrow \; c'_2}
    {(\textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2) \times c \; \rightarrow \;
    \textbf{if} \; b \; \textbf{then} \; c'_1 \; \textbf{else} \; c'_2}  
    
  \end{mathpar}
  \caption{Product construction rules.}
  \label{fig:product_construction_equal_struct}
\end{figure}

The \hyperref[fig:product_construction_equal_struct]{figure} describes a set of rules to derive a product construction judgment of the type \emph{c$_1$ $\times$ c$_2$ $\rightarrow$ c.}
To make sure that \emph{c} simulates with precision the behavior of \emph{c$_1$ and c$_2$}, the construction of products introduces \textbf{assert} statements.
During the phase of the verification of the program, these validation constraints are viewed as local assertions and discharged.
Let us consider the rule that synchronizes two loops, for example: \textbf{assert}(b$_1$ $\Leftrightarrow$ b$_2$) is necessary to guarantee that the number of iterations of each loop is the same.
This achieves what we aimed for, that is, the product program can be verified with standard logic.
\bigskip

\textbf{Proposition 1 -}  \emph{For all statements $c_1$ and $c_2$, pre-condition $\phi$ and post-condition $\psi$, if $c_1$ $\times$ $c_2$ $\rightarrow$ c and $\vDash$ {$\phi$} c {$\psi$} then $\vDash$ {$\phi$} $c_1$ \textasciitilde $c_2$ {$\psi$}.}

\bigskip
In other words, if \emph{c} is the resulting product program of \emph{c1} and \emph{c2}, then we can reason about the validity of the relational judgment between \emph{c1} and \emph{c2} through the validity of the standard judgment of \emph{c}.

As we mentioned before, the rules present in the previous \hyperref[fig:product_construction_equal_struct]{figure} are limited to structurally equivalent programs.
For example, if we consider two programs each with a loop and their guards are not equivalent, the product would have to be sequentially composed.
The structural translations proposed extend the construction of the already defined products in the form of a refinement relation, with a judgment of the form \emph{c} $\succcurlyeq$ \emph{c'}. 
This means that every execution of \emph{c} is an execution of \emph{c'} except when the latter's execution does not terminate:
\bigskip

\textbf{Definition 3 -}  \emph{A command c' is a refinement of c, if, for all states $\sigma$, $\sigma$':}    
\begin{itemize}
  \item{\emph{1. if $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$' then $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$'}}
  \vspace{-10pt}
  \item{\emph{2. if $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' then either the execution of c' with initial state $\sigma$ gets stuck, or $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$'.}}
\end{itemize}

\bigskip
In the \hyperref[fig:product_construction_reduction]{figure} is described the set of rules that define judgments of the form $\vdash$ \emph{c} $\succcurlyeq$ \emph{c'}.

\begin{figure}[h]
  \centering
  \begin{mathpar}

  \inferrule*[]
    { }
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{assert}(b); \; c_1}
    
  \inferrule*[]
    { }
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{assert}(\neg b); \; c_2}

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{assert}(b); \; c; \; \textbf{while} \; b \; \textbf{do} \; c}

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{while} \; b \; \land \; b' \; \textbf{do} \; c}  

  \inferrule*[]
    { }
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \textbf{assert}(b); \; c; \; \textbf{assert}(\neg b)}

  \inferrule*[]
    {\vdash c \; \succcurlyeq \; c'}
    {\vdash \textbf{while} \; b \; \textbf{do} \; c \; \succcurlyeq \; \vdash \textbf{while} \; b \; \textbf{do} \; c'}

  \inferrule*[]
    {\vdash c_1 \; \succcurlyeq \; c'_1 \\ \vdash c_2 \; \succcurlyeq \; c'_2}
    {\vdash \textbf{if} \; b \; \textbf{then} \; c_1 \; \textbf{else} \; c_2 \; \succcurlyeq \; \textbf{if} \; b \; \textbf{then} \; c'_1 \; \textbf{else} \; c'_2}

  \inferrule*[]
    {\vdash c \; \succcurlyeq \; c' \\ \vdash c' \; \succcurlyeq \; c''}
    {\vdash c \; \succcurlyeq \; c''}

  \inferrule*[]
    { }
    {\vdash c \; \succcurlyeq \; c}

  \inferrule*[]
    {\vdash c_1 \; \succcurlyeq \; c'_1 \\ \vdash c_2 \; \succcurlyeq \; c'_2}
    {\vdash c_1 ; c_2 \; \succcurlyeq \; c'_1 ; c'_2}
    
  \end{mathpar}
  \caption{Syntactic reduction rules.}
  \label{fig:product_construction_reduction}
\end{figure}

One can see that the \hyperref[fig:product_construction_reduction]{rules} point that the executions of \emph{c} and \emph{c'} match for every initial state that makes the introduced \textbf{assert} statements valid.
It is possible to prove that the judgment \emph{c} $\succcurlyeq$ \emph{c'} maintains a refinement relation by showing that, for every assertion $\phi$, if c' is $\phi$-nonstuck, then for all $\sigma$ $\in$ $\llbracket$$\phi$$\rrbracket$ such that $\langle$c, $\sigma$$\rangle$ $\Downarrow$ $\sigma$' we have $\langle$c', $\sigma$$\rangle$ $\Downarrow$ $\sigma$'.
We add one more rule that makes a preliminary refinement transformation over the product components:

\begin{figure}[H]
  \begin{center}
  \begin{minipage}{\linewidth}
  \centering
    \begin{mathpar}
      \inferrule*[]
        {\vdash c_1 \; \succcurlyeq \; c'_1 \\
        \vdash c_2 \; \succcurlyeq \; c'_2 \\
        c'_1 \times c'_2 \; \rightarrow \; c}
        {c_1 \times c_2 \; \rightarrow \; c}
    \end{mathpar}
  \end{minipage}
  \end{center}
\end{figure}

This does not invalidate \textbf{Proposition 1}. 
Finally, we formally reduce the problem of proving the validity of a relational judgment into the construction of the product program followed by the standard verification of that same product program.
\bigskip

\textbf{Proposition 2 -}  \emph{For all statements $c_1$ and $c_2$, pre-condition $\phi$ and post-condition $\psi$, if $c_1$ $\times$ $c_2$ $\rightarrow$ c and $\vdash$ {$\phi$} c {$\psi$} then $\vDash$ {$\phi$} $c_1$ \textasciitilde $c_2$ {$\psi$}.}

\bigskip


\FloatBarrier
\subsection{Examples} 
\label{subsec:product_programs_examples}

Although product programs can be applied to several relational properties, such as non-interference and continuity, we will focus our examples on the correctness of program optimizations.

\subsubsection{Loop alignment} 
\label{subsubsec:product_programs_loop_alignment}

Loop alignment is an optimization that consists in improving cache effectiveness by increasing the proximity of the memory locations accessed in each iteration of the loop.
\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Source program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 1@; 
        while (@i (*$\textcolor{red}{\leq}$*) N@) do
          @b[i] := a[i]@;
          @d[i] := b[i-1]@;
          @i++@
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Optimized program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        .j := 1.;
        .(*$\textcolor{blue}{\overline{d}}$*)[1] := (*$\textcolor{blue}{\overline{b}}$*)[0].;
        while (.j (*$\textcolor{blue}{\leq}$*) N-1.) do
          .(*$\textcolor{blue}{\overline{b}}$*)[j] := (*$\textcolor{blue}{\overline{a}}$*)[j].;
          .(*$\textcolor{blue}{\overline{d}}$*)[j+1] := (*$\textcolor{blue}{\overline{b}}$*)[j].;
          .j++.;
        .(*$\textcolor{blue}{\overline{b}}$*)[N] := (*$\textcolor{blue}{\overline{a}}$*)[N].
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.9\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Product program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        {a = (*$\overline{a}$*) (*$\land$*) b[0] = (*$\overline{b}$*)[0]}
          @i := 1@; 
          .j := 1.;
          assert(@i (*$\textcolor{red}{\leq}$*) N@);
          @b[i] := a[i]@; @d[i] := b[i-1]@; @i++@;
          .(*$\textcolor{blue}{\overline{d}}$*)[1] := (*$\textcolor{blue}{\overline{b}}$*)[0].;
          assert(@i (*$\textcolor{red}{\leq}$*) N@ (*$\Leftrightarrow$*) .j (*$\textcolor{blue}{\leq}$*) N-1.);
          while (@i (*$\textcolor{red}{\leq}$*) N@) do
            @b[i] := a[i]@; @d[i] := b[i-1]@; @i++@;
            .(*$\textcolor{blue}{\overline{b}}$*)[j] := (*$\textcolor{blue}{\overline{a}}$*)[j].; .(*$\textcolor{blue}{\overline{d}}$*)[j+1] := (*$\textcolor{blue}{\overline{b}}$*)[j].; .j++.;
            assert(@i (*$\textcolor{red}{\leq}$*) N@ (*$\Leftrightarrow$*) .j (*$\textcolor{blue}{\leq}$*) N-1.);
          .(*$\textcolor{blue}{\overline{b}}$*)[N] := (*$\textcolor{blue}{\overline{a}}$*)[N].
        {d[1,N] = (*$\overline{d}$*)[1,N]}
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \caption{Loop alignment.}
  \label{fig:loop_alignment}
\end{figure}

Consider \emph{N} $\geq$ 1.
In the source \hyperref[fig:loop_alignment]{program}, note that, in each iteration, the array \emph{b} is accessed twice; firstly in the write to index \emph{i} and secondly in the read of position \emph{i-1}.
What loop alignment does is transform the access of different indexes of \emph{b} into only one index, in this case, \emph{i}.
The corresponding product program starts by ensuring that the number of iterations of the loop is the same for the source and optimized programs, through the assignment statement \emph{$\overline{d}$[1] := $\overline{b}$[0]}.
To be verified, the program product needs a pre-condition, a post-condition and a loop invariant.
The pre-condition is \emph{a = $\overline{a}$ $\land$ b[0] := $\overline{b}$[0]}.
The post-condition is \emph{d[1, N] = $\overline{d}$[1, N]}, meaning that, for the indexes in the interval [1, \emph{N}], the values of the arrays \emph{d} and \emph{$\overline{d}$} are the same.
A suitable loop invariant is \emph{d[1, i) = $\overline{d}$[1, i)  $\land$  b[j] = a[j]  $\land$  $\overline{b}$[i] = b[i]  $\land$  i = j + 1}.
This specification guarantees that, if the input arrays \emph{a} and \emph{b} are equal, the values present in the array \emph{d} are the same when the execution of both product components terminates.

\FloatBarrier
\subsubsection{Induction variable strength reduction} 
\label{subsubsec:product_programs_strength_reduction}

Product programs allow the verification of optimizations that \emph{maintain} the control flow of programs, but simply modify the basic blocks.
Some rather common examples of this are constant propagation and common subexpression elimination.

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Source program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 0@; 
        while (@i < N@) do
          @j := i * B + C@;
          @x += j@;
          @i++@
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Optimized program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        .i(*\textcolor{blue}{\textasciiacute}*) := 0.;
        .j(*\textcolor{blue}{\textasciiacute}*) := C.;
        while (.i(*\textcolor{blue}{\textasciiacute}*) < N.) do
          .x(*\textcolor{blue}{\textasciiacute}*) += j(*\textcolor{blue}{\textasciiacute}*).;
          .j(*\textcolor{blue}{\textasciiacute}*) += B.;
          .i(*\textcolor{blue}{\textasciiacute}*)++.
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.9\textwidth}
    \begin{minipage}[t]{\linewidth}
      \textbf{Product program}
      \begin{lstlisting}[escapeinside={(*}{*)}, style=while_lang]
        @i := 0@; .i(*\textcolor{blue}{\textasciiacute}*) := 0.; .j(*\textcolor{blue}{\textasciiacute}*) := C.;
        while (@i < N@ (*$\land$*) .i(*\textcolor{blue}{\textasciiacute}*) < N.) do
          @j := i * B + C@; @x += j@; @i++@
          .x(*\textcolor{blue}{\textasciiacute}*) += j(*\textcolor{blue}{\textasciiacute}*).; .j(*\textcolor{blue}{\textasciiacute}*) += B.; .i(*\textcolor{blue}{\textasciiacute}*)++.
      \end{lstlisting}
    \end{minipage}
  \end{subfigure}
  \caption{Induction variable strength reduction.}
  \label{fig:induction_var_strength_red}
\end{figure}

The \hyperref[fig:induction_var_strength_red]{figure} demonstrates the effects that applying strength reduction has in a small program.
In the example, \emph{j} is a derived induction variable defined as a linear function on the induction variable \emph{i}.
The optimization in question substitutes the statement \emph{j := i * B + C} by the equivalent and more performant statement \emph{j' += B} (multiplication is more costly than addition to a computer), makes the assignment \emph{x' += j'} come first inside the loop and adds an initial assignment \emph{j' = C} before the start of the loop.
To verify the correctness of the optimization, we need to guarantee that \emph{x = x'} is an invariant of the product program.
And to do that, we need the linear condition \emph{j := i * B + C} to be part of the loop invariant.

\FloatBarrier
\section{WhyRel}
\label{sec:whyrel}

\subsection{Introduction}
\label{subsec:whyrel_intro}

WhyRel~\cite{whyrel} is a tool that aims to verify relational properties of pointer programs based on relational region logic, in an auto-active way.
It also supports state-based encapsulation and dynamic framing. 
This tool appears in a context where there are two existing types of tools exist but are not powerful enough to reduce the gap between them.
One of the types of tools are the ones that do not support well the relational verification of programs that dynamically allocate mutable state; these take much weight off users' shoulders but are only useful to a restrict range of programs.
The other type of tools are the auto-active ones; these enable the verification of a considerably larger set of programs but require more user input.

Some of the most relevant relational properties are conditional program equivalence, non-interference and sensitivity~\cite{barthe2019verifying}.
--- TODO: PAGE 2---

This tool should be interpreted as a front-end to Why3, where users provide the code, specifications and annotations and, additionally for relational verification, they should also provide relational specifications and alignments using a syntax for product programs.
After that, WhyRel translates these inputs into WhyML code, enconding them in a way that accurately captures the representation of the heap model and fine-grained framing formalized in relational region logic.
Fianlly, some VCs are added by WhyRel as intermediate assertions and lemmas for the user to confirm, and the verification itself is achieved mainly through the use of the Why3 IDE.

WhyRel was evaluated through several case studies that demonstrate the effectiveness of relational region logic for alignment, expressing heap relations and relational reasoning that exploits encapsulation.


\subsection{Specifying programs and relational properties}
\label{subsec:whyrel_specifying}

a tour of WhyRel


\subsection{Patterns of program alignment}
\label{subsec:whyrel_patterns}

Patterns of alignment


\subsection{Translating biprograms to product programs}
\label{subsec:whyrel_translation}

encoding and design
